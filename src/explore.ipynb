{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EXPLORED EDA - NLP"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## IMPORTS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ---------------------------------------------------------\n",
                "# GENERAL IMPORTS\n",
                "# ---------------------------------------------------------\n",
                "# CORE PYTHON & DATA MANIPULATION LIBRARIES\n",
                "import numpy as np                # Numerical computations, arrays, math operations\n",
                "import pandas as pd               # Data handling, DataFrames, time-series structures\n",
                "import warnings                   # Warning control and suppression\n",
                "import math                       # Math utilities (sqrt, log, floor, ceil, etc.)\n",
                "# VISUALIZATION LIBRARIES\n",
                "import matplotlib.pyplot as plt   # Main plotting library\n",
                "import seaborn as sns             # Statistical and enhanced visualization tools\n",
                "# DATA SPLIT UTILITIES\n",
                "from sklearn.model_selection import train_test_split   # Split dataset into train / test subsets\n",
                "# MODEL SELECTION\n",
                "from sklearn.model_selection import GridSearchCV       # Hyperparameter optimization via grid search\n",
                "from sklearn.tree import plot_tree                     # Visualization of decision tree structures\n",
                "from pickle import dump                                # Save trained models to disk (serialization)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# CLASSIFICATION ALGORITHMS\n",
                "# ---------------------------------------------------------\n",
                "# METRICS\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score   # Core classification metrics\n",
                "# PREDICTION MODELS\n",
                "from sklearn.linear_model import LogisticRegression     # Logistic regression classifier\n",
                "from sklearn.tree import DecisionTreeClassifier         # Decision tree classifier\n",
                "from sklearn.naive_bayes import GaussianNB              # Gaussian Naive Bayes for continuous inputs\n",
                "from sklearn.naive_bayes import MultinomialNB           # Multinomial Naive Bayes (common for NLP)\n",
                "from sklearn.naive_bayes import BernoulliNB             # Bernoulli Naive Bayes (binary features)\n",
                "# BAGGING ENSEMBLE\n",
                "from sklearn.ensemble import RandomForestClassifier     # Ensemble of decision trees (bagging)\n",
                "# BOOSTING ENSEMBLE\n",
                "from sklearn.ensemble import AdaBoostClassifier         # AdaBoost boosting algorithm\n",
                "from sklearn.ensemble import GradientBoostingClassifier # Gradient boosting classifier\n",
                "from xgboost import XGBClassifier                       # Extreme Gradient Boosting (high-performance)\n",
                "from lightgbm import LGBMClassifier                     # LightGBM (optimized gradient boosting)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# REGRESSION ALGORITHMS\n",
                "# ---------------------------------------------------------\n",
                "# METRICS\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score   # Regression performance metrics\n",
                "# PREDICTION MODELS\n",
                "from sklearn.linear_model import LinearRegression       # Linear regression model\n",
                "from sklearn.linear_model import Lasso                  # L1 regularized regression\n",
                "from sklearn.linear_model import Ridge                  # L2 regularized regression\n",
                "from sklearn.tree import DecisionTreeRegressor          # Regression decision tree\n",
                "# BAGGING ENSEMBLE\n",
                "from sklearn.ensemble import RandomForestRegressor      # Ensemble of regression trees (bagging)\n",
                "# BOOSTING ENSEMBLE\n",
                "from sklearn.ensemble import AdaBoostRegressor          # Boosting algorithm for regression\n",
                "from sklearn.ensemble import GradientBoostingRegressor  # Gradient boosting regressor\n",
                "from xgboost import XGBRegressor                        # XGBoost regressor\n",
                "from lightgbm import LGBMRegressor                      # LightGBM regressor\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# NLP DATASETS - EDA\n",
                "# ---------------------------------------------------------\n",
                "import regex as re\n",
                "\n",
                "\n",
                "# STEP 13) VECTORIZATION\n",
                "from sklearn.feature_extraction.text import CountVectorizer     # Convert text into token frequency counts\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer     # Convert text into TF-IDF weighted features\n",
                "from sklearn.decomposition import PCA                           # Dimensionality reduction (e.g., for visualization)\n",
                "from sklearn.metrics import silhouette_score                    # Clustering quality metric (optional for NLP clustering)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# TIME-SERIES DATASETS - EDA\n",
                "# ---------------------------------------------------------\n",
                "# STEP 3) DECOMPOSING\n",
                "from statsmodels.tsa.seasonal import seasonal_decompose         # Decompose time-series into trend, seasonal, and residual components\n",
                "# STEP 4) STATIONARITY ANALYSIS\n",
                "from statsmodels.tsa.stattools import adfuller                  # Dickey-Fuller test for stationarity evaluation\n",
                "# STEP 5) VARIABILITY ANALYSIS\n",
                "from statsmodels.stats.diagnostic import acorr_ljungbox         # Ljung-Box test for checking autocorrelation in residuals\n",
                "# STEP 6) AUTOCORRELATION ANALYSIS\n",
                "from statsmodels.tsa.stattools import acf                       # Compute autocorrelation values  \n",
                "from statsmodels.tsa.stattools import pacf                      # Compute partial autocorrelation values\n",
                "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf   # Plot ACF & PACF with confidence bands\n",
                "# STEP 7) PREDICTION MODEL\n",
                "from statsmodels.tsa.arima.model import ARIMA                   # ARIMA(p,d,q) model for forecasting\n",
                "from pmdarima import auto_arima                                 # Automatic ARIMA/SARIMA parameter selection\n",
                "from statsmodels.tools.sm_exceptions import ConvergenceWarning  # Warning raised when ARIMA optimizer fails\n",
                "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)  # Suppress convergence warnings globally (keeps logs clean)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# TABULAR DATASETS - EDA\n",
                "# ---------------------------------------------------------\n",
                "# STEP 10) REMOVE NOISY ATTRIBUTES\n",
                "from scipy.stats import chi2_contingency                # Chi-square test for categorical dependencies\n",
                "# STEP 12) SCALLING\n",
                "from sklearn.preprocessing import StandardScaler        # Standardization (mean=0, std=1)\n",
                "from sklearn.preprocessing import MinMaxScaler          # Min-max scaling to [0,1]\n",
                "# STEP 13) ENCODING\n",
                "from sklearn.preprocessing import LabelEncoder          # Encode categories into integer labels\n",
                "from sklearn.preprocessing import OneHotEncoder         # Encode categories into binary vectors\n",
                "# STEP 14) FEATURE SELECTION\n",
                "from sklearn.feature_selection import SelectKBest, f_classif   # Univariate feature selection for classification"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## HELPER FUNCTIONS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ---------------------------------------------------------\n",
                "# COMMON\n",
                "# ---------------------------------------------------------\n",
                "# Log printer\n",
                "def log(message: str, level: int = 1, type: str = \"INFO\", custom_icon: str = None, bold: bool = False):\n",
                "    # Default icons according to message type\n",
                "    icons = {\n",
                "        \"INFO\": \"‚ÑπÔ∏è\",\n",
                "        \"FOUND\": \"üîç\",\n",
                "        \"SUCCESS\": \"‚úÖ\",\n",
                "        \"ERROR\": \"‚ùå\",\n",
                "        \"WARNING\": \"‚ö†Ô∏è\",\n",
                "    }\n",
                "    # Use custom icon if provided\n",
                "    if custom_icon:\n",
                "        icon = custom_icon\n",
                "    else:\n",
                "        icon = icons.get(type.upper(), \"‚ÑπÔ∏è\")\n",
                "    # Bold wrapper (ANSI)\n",
                "    if bold:\n",
                "        message = f\"\\033[1m{message}\\033[0m\"\n",
                "    # First level ‚Üí bullet\n",
                "    if level == 1:\n",
                "        prefix = \"‚Ä¢\"\n",
                "    # Second level ‚Üí indent + hyphen\n",
                "    elif level == 2:\n",
                "        prefix = \"   -\"\n",
                "    # Level 3 ‚Üí deeper indent + middle dot\n",
                "    elif level == 3:\n",
                "        prefix = \"      ¬∑\"\n",
                "    # Fallback\n",
                "    else:\n",
                "        prefix = \"-\"\n",
                "    # Final print\n",
                "    print(f\"{prefix} {icon} {message}\")\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# TIME-SERIES DATASETS - EDA\n",
                "# ---------------------------------------------------------\n",
                "# Determines granularity given seconds\n",
                "def determine_granularity(seconds: float):\n",
                "    if seconds < 1:\n",
                "        granularity = \"sub-second\"\n",
                "    elif seconds >= 1 and seconds < 60:\n",
                "        granularity = \"second\"\n",
                "    elif seconds >= 60 and seconds < 3600:\n",
                "        granularity = \"minute\"\n",
                "    elif seconds >= 3600 and seconds < 86400:\n",
                "        granularity = \"hour\"\n",
                "    elif seconds == 86400:\n",
                "        granularity = \"day\"\n",
                "    elif seconds > 86400 and seconds <= 86400 * 7:\n",
                "        granularity = \"multi-day\"\n",
                "    elif seconds == 86400 * 7:\n",
                "        granularity = \"week\"\n",
                "    elif seconds > 86400 * 7 and seconds < 86400 * 28:\n",
                "        granularity = \"weekly-to-monthly\"\n",
                "    elif seconds >= 86400 * 28 and seconds <= 86400 * 31:\n",
                "        granularity = \"month\"\n",
                "    elif seconds > 86400 * 31 and seconds <= 86400 * 92:\n",
                "        granularity = \"quarter\"\n",
                "    else:\n",
                "        granularity = \"year-or-more\"\n",
                "    return granularity\n",
                "\n",
                "# Detects whether the seasonal decomposition model should be\n",
                "# 'additive' (constant amplitude) or \n",
                "# 'multiplicative'(amplitude grows with the trend)\n",
                "def infer_seasonal_component_type(series: pd.Series, threshold: float) -> str:\n",
                "    # Compute amplitude and mean\n",
                "    amplitude = series.max() - series.min()\n",
                "    mean_val = series.mean()\n",
                "    # Edge case: zero-mean ‚Üí multiplicative impossible\n",
                "    if mean_val == 0:\n",
                "        return \"additive\"\n",
                "    # Threshold for deciding multiplicative\n",
                "    if amplitude / abs(mean_val) > threshold:\n",
                "        return \"multiplicative\"\n",
                "    else:\n",
                "        return \"additive\"\n",
                "\n",
                "#  Detects seasonality period automatically using the first significant ACF peak\n",
                "def infer_period_from_acf(series: pd.Series, max_lag_ratio: float = 0.1):\n",
                "    n = len(series)\n",
                "    max_lag = max(5, int(n * max_lag_ratio))\n",
                "    # Compute autocorrelation using FFT\n",
                "    autocorr = acf(series, nlags=max_lag, fft=True, missing=\"drop\")\n",
                "    # Ignore lag 0\n",
                "    autocorr[0] = 0\n",
                "    # Find the highest correlation peak\n",
                "    peak_lag = np.argmax(autocorr)\n",
                "    # If the peak is too low ‚Üí no seasonality\n",
                "    if autocorr[peak_lag] < 0.3:\n",
                "        return None\n",
                "    return peak_lag\n",
                "\n",
                "#  Detects seasonality period automatically using Granularity (fallback when ACF fails or data is too noisy)\n",
                "def infer_period_from_granularity(granularity: str):\n",
                "    if granularity == \"sub-second\":\n",
                "        return None\n",
                "    if granularity == \"second\":\n",
                "        return 60\n",
                "    if granularity == \"minute\":\n",
                "        return 60\n",
                "    if granularity == \"hour\":\n",
                "        return 24\n",
                "    if granularity == \"day\":\n",
                "        return 7  # Most common weekly cycle IF the series shows any seasonality\n",
                "    if granularity == \"multi-day\":\n",
                "        return 7\n",
                "    if granularity == \"week\":\n",
                "        return 52\n",
                "    if granularity == \"weekly-to-monthly\":\n",
                "        return 12\n",
                "    if granularity == \"month\":\n",
                "        return 12\n",
                "    if granularity == \"quarter\":\n",
                "        return 4\n",
                "    return None # Yearly or undefined ‚Üí no decomposition\n",
                "\n",
                "# 6) Evaluates how strong the seasonality is using:\n",
                "# 1) Variance ratio: Var(seasonal) / Var(original)\n",
                "# 2) ACF at the seasonal period\n",
                "def assess_seasonality_strength(original: pd.Series, seasonal: pd.Series, period: int, acf_threshold: float, var_ratio: float):\n",
                "    # Align indices and remove NaN values from the seasonal component\n",
                "    valid_mask = seasonal.notna()\n",
                "    original_valid = original[valid_mask]\n",
                "    seasonal_valid = seasonal[valid_mask]\n",
                "    # If there are not enough valid points ‚Üí cannot assess\n",
                "    if len(original_valid) < max(10, period * 2):\n",
                "        metrics = {\n",
                "            \"seasonal_var_ratio\": np.nan,\n",
                "            \"acf_at_period\": np.nan\n",
                "        }\n",
                "        return False, metrics\n",
                "    # 1) Variance ratio\n",
                "    total_var = np.var(original_valid)\n",
                "    seasonal_var = np.var(seasonal_valid)\n",
                "    if total_var == 0:\n",
                "        seasonal_var_ratio = 0.0\n",
                "    else:\n",
                "        seasonal_var_ratio = seasonal_var / total_var\n",
                "    # 2) ACF at seasonal period\n",
                "    acf_values = acf(\n",
                "        original_valid,\n",
                "        nlags = period,\n",
                "        fft = True,\n",
                "        missing = \"drop\"\n",
                "    )\n",
                "    acf_at_period = acf_values[period]\n",
                "    # 3) Decision rule\n",
                "    strong_seasonality = ((seasonal_var_ratio >= var_ratio) and (acf_at_period >= acf_threshold))\n",
                "    # 4) Metrics\n",
                "    metrics = {\n",
                "        \"seasonal_var_ratio\": seasonal_var_ratio,\n",
                "        \"acf_at_period\": acf_at_period\n",
                "    }\n",
                "    return strong_seasonality, metrics\n",
                "\n",
                "# Performs Dickey-Fuller test to determine if a series is stacionary or not\n",
                "def test_stationarity(series):\n",
                "    dftest = adfuller(series, autolag = \"AIC\")\n",
                "    dfoutput = pd.Series(dftest[0:4], index = [\"Test Statistic\", \"p-value\", \"#Lags Used\", \"Number of Observations Used\"])\n",
                "    for key,value in dftest[4].items():\n",
                "        dfoutput[\"Critical Value (%s)\"%key] = value\n",
                "    return dfoutput\n",
                "\n",
                "# Recursively differences the time-series until Dickey-Fuller test accepts stationarity (p < alpha)\n",
                "def make_stationary_recursive(series, alpha: float = 0.05, max_diff: int = 5):\n",
                "    current_series = series.copy()\n",
                "    diff_count = 0\n",
                "    while diff_count <= max_diff:\n",
                "        test_results = test_stationarity(current_series)\n",
                "        if test_results[\"p-value\"] < alpha:\n",
                "            return current_series, diff_count, test_results\n",
                "        current_series = current_series.diff().dropna()\n",
                "        diff_count += 1\n",
                "    # If exceeded max_diff ‚Üí return last attempt\n",
                "    return current_series, diff_count, test_results\n",
                "\n",
                "# Function to get recommended lag based on granularity\n",
                "def get_recommended_lag(granularity):\n",
                "    if granularity == \"sub-second\" or granularity == \"second\":\n",
                "        return 600   # Captures 10 minutes of autocorrelation\n",
                "    if granularity == \"minute\":\n",
                "        return 300   # Captures 5 hours\n",
                "    if granularity == \"hour\":\n",
                "        return 200   # Captures up to weekly cycles (168h)\n",
                "    if granularity == \"day\":\n",
                "        return 60    # Enough for weekly + monthly seasonality\n",
                "    if granularity == \"multi-day\":\n",
                "        return 60\n",
                "    if granularity == \"week\":\n",
                "        return 60    # Enough to detect annual cycle (52 weeks)\n",
                "    if granularity == \"month\":\n",
                "        return 48    # Captures 4 years of monthly pattern\n",
                "    if granularity == \"quarter\":\n",
                "        return 20\n",
                "    return 10        # Yearly or undefined ‚Üí very small possible lags\n",
                "\n",
                "# Function to get recommended cutoff for short-lag autocorrelation detection based on granularity\n",
                "def get_short_lag_cutoff(granularity):\n",
                "    if granularity == \"sub-second\" or granularity == \"second\":\n",
                "        return 30   # 30 seconds of persistence\n",
                "    if granularity == \"minute\":\n",
                "        return 60   # 1 hour of short-term memory\n",
                "    if granularity == \"hour\":\n",
                "        return 24   # One day's worth of lags\n",
                "    if granularity == \"day\":\n",
                "        return 14   # Two weeks ‚Üí enough to detect trend\n",
                "    if granularity == \"multi-day\":\n",
                "        return 10\n",
                "    if granularity == \"week\":\n",
                "        return 8    # ~2 months of weekly persistence\n",
                "    if granularity == \"month\":\n",
                "        return 12   # 1 year of autocorrelation\n",
                "    if granularity == \"quarter\":\n",
                "        return 8\n",
                "    return 3        # Very limited short-term interpretation\n",
                "\n",
                "# Return a pandas-compatible frequency string based on granularity\n",
                "def get_freq_from_granularity(granularity: str):\n",
                "    freq_map = {\n",
                "        \"sub-second\": None,             # No valid pandas freq for < 1 second\n",
                "        \"second\": \"S\",                  # Second-level frequency\n",
                "        \"minute\": \"T\",                  # Minute-level frequency\n",
                "        \"hour\": \"H\",                    # Hourly frequency\n",
                "        \"day\": \"D\",                     # Daily frequency\n",
                "        \"multi-day\": \"D\",               # Best stable approximation\n",
                "        \"week\": \"W\",                    # Weekly frequency\n",
                "        \"weekly-to-monthly\": \"W\",       # Ambiguous ‚Üí weekly fits more stable\n",
                "        \"month\": \"M\",                   # Monthly frequency\n",
                "        \"quarter\": \"Q\",                 # Quarterly frequency\n",
                "        \"year-or-more\": \"A\"             # Annual frequency\n",
                "    }\n",
                "    if granularity not in freq_map:\n",
                "        return None\n",
                "    return freq_map[granularity]\n",
                "\n",
                "# Get seasonal period m for auto_arima\n",
                "def get_auto_arima_m(period: int, seasonal_peaks: list):\n",
                "    if period is None:\n",
                "        return 1\n",
                "    if seasonal_peaks is None:\n",
                "        return 1\n",
                "    if len(seasonal_peaks) == 0:\n",
                "        return 1\n",
                "    if period >= 2:\n",
                "        return int(period)\n",
                "    return 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## COMMON INPUTS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plotting\n",
                "num_values_to_plot = 40     # Max number of different values to plot (for CATEGORY_var)\n",
                "num_bins = 100              # Num of bins (for NUMERIC_var plots)\n",
                "figHeight_unit = 8\n",
                "figWidth_unit = 12\n",
                "plot_palette = \"pastel\"\n",
                "plot_tick_font_size = 15\n",
                "plot_label_font_size = 15\n",
                "plot_text_font_size = 20\n",
                "plot_title_font_size = 30"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 0 - LOAD RAW DATAFRAME"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Raw dataset formatting\n",
                "data_separator = \",\"\n",
                "input_path = \"../data/raw/internal-link.csv\"\n",
                "# Thresholds for dataset type proposal (NLP)\n",
                "min_text_avg_length = 25      # Recommended: 20‚Äì30 chars ‚Üí typical minimum for real text\n",
                "min_text_avg_words  = 3       # Recommended: >3 words ‚Üí avoids titles/labels\n",
                "min_points_nlp = 70           # Min points to be considered NLP dataset (max point = 100)\n",
                "# Thresholds for dataset type proposal (Time-Series)\n",
                "ts_main_col_index = 0         # Index of the datetime column to be used as the primary time axis\n",
                "min_rows_in_dataset = 75      # Recommended: > 75\n",
                "max_numeric_var = 5           # More than 5 is odd for a time-series\n",
                "freq_ratio_threshold = 0.7    # Recommended: > 0.7 (values from 0 to 1)\n",
                "min_points_ts = 70            # Min points to be considered Time-Series dataset (max point = 100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "-------------------------------\n",
                        "STEP 0) LOAD RAW DATAFRAME\n",
                        "-------------------------------\n",
                        "\n",
                        "‚Ä¢ ‚úÖ DataFrame loaded successfully!\n",
                        "‚Ä¢ üìù NLP cheking:\n",
                        "   - ‚úÖ Object-type columns found: ['url'] (+25 points)\n",
                        "   - ‚úÖ Long text-like columns found: ['url'] (+25 points)\n",
                        "   - ‚ùå No column shows word-rich entries\n",
                        "   - ‚úÖ There is only 1 long text-like column, it is typical for NLP (+15 points)\n",
                        "   - ‚úÖ Majority of rows contain meaningful text (+10 points)\n",
                        "   - ‚ÑπÔ∏è \u001b[1mScore to be a NLP Dataset: 75/100 points\u001b[0m\n",
                        "‚Ä¢ ‚è±Ô∏è TIME-SERIES checking:\n",
                        "   - ‚ùå No datetime columns detected\n",
                        "   - ‚ÑπÔ∏è \u001b[1mScore to be a TIME-SERIES Dataset: 0/100 points\u001b[0m\n",
                        "\n",
                        "\n",
                        "‚Ä¢ ‚ÑπÔ∏è \u001b[1mProposed dataset type: NLP\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "print(\"-------------------------------\")\n",
                "print(\"STEP 0) LOAD RAW DATAFRAME\")\n",
                "print(\"-------------------------------\\n\")\n",
                "\n",
                "# Load raw DataFrame\n",
                "df_raw = pd.read_csv(input_path, sep=data_separator)\n",
                "log(\"DataFrame loaded successfully!\", type=\"SUCCESS\")\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# NLP CHECKING (probabilistic)\n",
                "# ---------------------------------------------------------\n",
                "log(\"NLP cheking:\", custom_icon=\"üìù\")\n",
                "nlp_score = 0            # Final probability score (0‚Äì100)\n",
                "# Evidence 1: dataset has at least one text-like column\n",
                "object_cols = []\n",
                "for col in df_raw.columns:\n",
                "    if df_raw[col].dtype in [\"object\", \"category\"]:\n",
                "        object_cols.append(col)\n",
                "if len(object_cols) >= 1:\n",
                "    nlp_score += 25\n",
                "    log(f\"Object-type columns found: {object_cols} (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "else:\n",
                "    log(\"No object-type columns found\", level=2, type=\"ERROR\")\n",
                "# Evidence 2: long text columns exist\n",
                "text_cols = []\n",
                "for col in object_cols:\n",
                "    avg_len = df_raw[col].dropna().str.len().mean()\n",
                "    if avg_len is not None and avg_len > min_text_avg_length:\n",
                "        text_cols.append(col)\n",
                "if len(text_cols) >= 1:\n",
                "    nlp_score += 25\n",
                "    log(f\"Long text-like columns found: {text_cols} (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "else:\n",
                "    log(\"No long text-like columns found\", level=2, type=\"ERROR\")\n",
                "# Evidence 3: rich text structure (words per entry)\n",
                "rich_text_cols = []\n",
                "for col in text_cols:\n",
                "    avg_words = df_raw[col].dropna().str.split().str.len().mean()\n",
                "    if avg_words is not None and avg_words > min_text_avg_words:\n",
                "        rich_text_cols.append(col)\n",
                "if len(rich_text_cols) > 0:\n",
                "    nlp_score += 25\n",
                "    log(f\"At least one column shows word-rich entries (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "else:\n",
                "    log(\"No column shows word-rich entries\", level=2, type=\"ERROR\")\n",
                "# Evidence 4: number of text columns realistic for NLP\n",
                "if len(text_cols) == 1:\n",
                "    nlp_score += 15\n",
                "    log(f\"There is only 1 long text-like column, it is typical for NLP (+15 points)\", level=2, type=\"SUCCESS\")\n",
                "elif len(text_cols) > 1:\n",
                "    nlp_score += 5\n",
                "    log(f\"There are more than 1 long text-like column, it could be possible for NLP (+5 points)\", level=2, type=\"SUCCESS\")\n",
                "else:\n",
                "    log(\"No long text-like columns found\", level=2, type=\"ERROR\")\n",
                "# Evidence 5: proportion of non-empty text entries\n",
                "if len(text_cols) >= 1:\n",
                "    non_empty_ratio = df_raw[text_cols[0]].dropna().str.len().gt(10).mean()\n",
                "    if non_empty_ratio >= 0.6:\n",
                "        nlp_score += 10\n",
                "        log(f\"Majority of rows contain meaningful text (+10 points)\", level=2, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"Too many empty/short text entries\", level=2, type=\"ERROR\")\n",
                "# Cap score at 100\n",
                "nlp_score = min(nlp_score, 100)\n",
                "log(f\"Score to be a NLP Dataset: {nlp_score}/100 points\", level=2, type=\"INFO\", bold=True)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# TIME-SERIES CHECKING (probabilistic)\n",
                "# ---------------------------------------------------------\n",
                "log(\"TIME-SERIES checking:\", custom_icon=\"‚è±Ô∏è\")\n",
                "ts_score = 0                  # Final probability score (0‚Äì100)\n",
                "ts_cols = []                  # List of detected datetime columns\n",
                "# Evidence 1) Detect datetime columns\n",
                "warnings.filterwarnings(\"ignore\", message=\"Could not infer format\", category=UserWarning) # Suppress warnings only related to datetime inference\n",
                "for col in df_raw.columns:\n",
                "    try:\n",
                "        pd.to_datetime(df_raw[col], errors=\"raise\")\n",
                "        ts_cols.append(col)\n",
                "    except:\n",
                "        pass\n",
                "# Case 1 ‚Üí no datetime columns\n",
                "if len(ts_cols) == 0:\n",
                "    log(\"No datetime columns detected\", level=2, type=\"ERROR\")\n",
                "# Case 2 ‚Üí exactly one column\n",
                "elif len(ts_cols) == 1:\n",
                "    ts_score += 40\n",
                "    log(f\"Unique datetime column detected: {ts_cols[ts_main_col_index]} (+40 points)\", level=2, type=\"SUCCESS\")\n",
                "# Case 3 ‚Üí multiple datetime columns\n",
                "elif len(ts_cols) > 1:\n",
                "    ts_score += 25\n",
                "    log(f\"Several datetime columns were detected: {ts_cols} (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "    # Check if all datetime columns share similar structure\n",
                "    for col in ts_cols:\n",
                "        try:\n",
                "            dt_tmp = pd.to_datetime(df_raw[col], errors=\"coerce\")\n",
                "            missing_rate = dt_tmp.isna().mean()\n",
                "            log(f\"Column '{col}' parsed with missing rate: {missing_rate:.3f}\", level=3, type=\"INFO\")\n",
                "        except:\n",
                "            log(f\"Column '{col}' failed advanced parsing\", level=3, type=\"ERROR\")\n",
                "if len(ts_cols) > 0: # Evaluate time-series structure (only if datetime column exists)\n",
                "    # Convert chosen datetime column\n",
                "    serie_date_time_raw = pd.to_datetime(df_raw[ts_cols[ts_main_col_index]], errors=\"coerce\")\n",
                "    # Evidence 2) Chronologically sorted\n",
                "    if serie_date_time_raw.is_monotonic_increasing:\n",
                "        ts_score += 20\n",
                "        log(f\"Datetime column '{ts_cols[ts_main_col_index]}' is sorted (+20 points)\", level=2, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(f\"Datetime column '{ts_cols[ts_main_col_index]}' is NOT sorted\", level=2, type=\"ERROR\")\n",
                "    # Evidence 3) Detecting time-series frequency\n",
                "    serie_date_time_diff_raw = serie_date_time_raw.diff().dropna()\n",
                "    if len(serie_date_time_diff_raw) > 0:\n",
                "        # Compute most common interval\n",
                "        most_common_delta = serie_date_time_diff_raw.mode()[0]\n",
                "        freq_ratio = (serie_date_time_diff_raw == most_common_delta).mean()\n",
                "        if freq_ratio >= freq_ratio_threshold:\n",
                "            ts_score += 20\n",
                "            log(f\"Regular frequency detected (+20 points)\", level=2, type=\"SUCCESS\")\n",
                "            log(f\"Frequency consistency ratio: {freq_ratio:.3f}\", level=3, type=\"INFO\")\n",
                "        else:\n",
                "            log(\"No regular frequency detected\", level=2, type=\"ERROR\")\n",
                "    else:\n",
                "        log(\"Not enough data to detect time-series frequency\", level=2, type=\"ERROR\")\n",
                "    # Evidence 4) Numeric columns over time\n",
                "    numeric_cols = df_raw.select_dtypes(include=[\"number\"]).columns\n",
                "    if 1 <= len(numeric_cols) <= max_numeric_var:\n",
                "        ts_score += 10\n",
                "        log(f\"Numeric variables suitable for TS (+10 points)\", level=2, type=\"SUCCESS\")\n",
                "    elif len(numeric_cols) < 1:\n",
                "        log(\"There is not at least one numeric variable for time-series\", level=2, type=\"ERROR\")\n",
                "    else:\n",
                "        log(\"Too many numeric varaibles for time-series\", level=2, type=\"ERROR\")\n",
                "    # Evidence 5) Dataset length\n",
                "    if len(df_raw) >= min_rows_in_dataset:\n",
                "        ts_score += 10\n",
                "        log(f\"Enough rows for time-series (+10 points)\", level=2, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"Dataset too short for time-series\", level=2, type=\"ERROR\")\n",
                "# Cap score at 100\n",
                "ts_score = min(ts_score, 100)\n",
                "log(f\"Score to be a TIME-SERIES Dataset: {ts_score}/100 points\", level=2, type=\"INFO\", bold=True)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# DATASET TYPE PROPOSAL\n",
                "# ---------------------------------------------------------\n",
                "if ts_score >= nlp_score and ts_score >= min_points_ts:\n",
                "    dataset_type_auto = \"TIME-SERIES\"\n",
                "elif nlp_score >= ts_score and nlp_score >= min_points_nlp:\n",
                "    dataset_type_auto = \"NLP\"\n",
                "else:\n",
                "    dataset_type_auto = \"TABULAR\"\n",
                "print(\"\\n\")\n",
                "log(f\"Proposed dataset type: {dataset_type_auto}\", type=\"INFO\", bold=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 1 - EXPLORE DATAFRAME"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_type = \"NLP\" # Confirm dataset type (TIME-SERIES, NLP or TABULAR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "-------------------------------\n",
                        "STEP 1) EXPLORE DATAFRAME\n",
                        "-------------------------------\n",
                        "\n",
                        "‚Ä¢ üìù Dataset detected as NLP (long-text dataset)\n",
                        "‚Ä¢ ‚ÑπÔ∏è Shape of the DataFrame: (2999, 2)\n",
                        "‚Ä¢ ‚ÑπÔ∏è Content of the DataFrame:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>url</th>\n",
                            "      <th>is_spam</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>https://www.hvper.com/</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>https://briefingday.com/n/20200618/m#commentform</td>\n",
                            "      <td>False</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>https://briefingday.com/fan</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 url  is_spam\n",
                            "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                            "1                             https://www.hvper.com/     True\n",
                            "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                            "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                            "4                        https://briefingday.com/fan     True"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚Ä¢ ‚ÑπÔ∏è Sample of raw text entries:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>url</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>https://www.hvper.com/</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>https://briefingday.com/n/20200618/m#commentform</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>https://briefingday.com/fan</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 url\n",
                            "0  https://briefingday.us8.list-manage.com/unsubs...\n",
                            "1                             https://www.hvper.com/\n",
                            "2                 https://briefingday.com/m/v4n3i4f3\n",
                            "3   https://briefingday.com/n/20200618/m#commentform\n",
                            "4                        https://briefingday.com/fan"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚Ä¢ ‚ÑπÔ∏è Text length statistics:\n",
                        "   - üìä Average lengths: 73.5 chars\n",
                        "   - üìä Median lengths: 71.0 chars\n",
                        "   - üìä Max lengths: 269.0 chars\n",
                        "‚Ä¢ ‚ÑπÔ∏è Word count statistics:\n",
                        "   - üìä Average words: 1.0 chars\n",
                        "   - üìä Median words: 1.0 chars\n",
                        "   - üìä Max words: 1.0 chars\n"
                    ]
                }
            ],
            "source": [
                "# Copy previous step data\n",
                "df_S1 = df_raw.copy()\n",
                "\n",
                "# -------------------------------\n",
                "# NLP DATASET\n",
                "# -------------------------------\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 1) EXPLORE DATAFRAME\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    log(\"Dataset detected as NLP (long-text dataset)\", custom_icon=\"üìù\")\n",
                "    # Print info\n",
                "    log(f\"Shape of the DataFrame: {df_S1.shape}\", type=\"INFO\")\n",
                "    log(\"Content of the DataFrame:\", type=\"INFO\")\n",
                "    display(df_S1.head(5))\n",
                "    log(\"Sample of raw text entries:\", type=\"INFO\")\n",
                "    display(df_S1[text_cols].head(5))\n",
                "    # Print text length stats\n",
                "    df_S1_lengths = df_S1[text_cols[0]].astype(str).str.len()\n",
                "    log(\"Text length statistics:\", type=\"INFO\")\n",
                "    log(f\"Average lengths: {df_S1_lengths.mean():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    log(f\"Median lengths: {df_S1_lengths.median():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    log(f\"Max lengths: {df_S1_lengths.max():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    # Compute word statistics\n",
                "    df_S1_words = df_S1[text_cols[0]].astype(str).str.split().str.len()\n",
                "    log(\"Word count statistics:\", type=\"INFO\")\n",
                "    log(f\"Average words: {df_S1_words.mean():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    log(f\"Median words: {df_S1_words.median():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    log(f\"Max words: {df_S1_words.max():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "# -------------------------------\n",
                "# TIME-SERIES DATASET\n",
                "# -------------------------------\n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 1) EXPLORE DATAFRAME\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    log(\"Dataset detected as TIME-SERIES\", custom_icon=\"‚è±Ô∏è\")\n",
                "    # Print info\n",
                "    log(f\"Shape of the DataFrame: {df_S1.shape}\", type=\"INFO\")\n",
                "    log(\"Content of the DataFrame:\", type=\"INFO\")\n",
                "    display(df_S1.head(5))\n",
                "    # Try to parse every column\n",
                "    df_S1_ts_cols = []\n",
                "    for col in df_S1.columns:\n",
                "        try:\n",
                "            pd.to_datetime(df_S1[col], errors=\"raise\")\n",
                "            df_S1_ts_cols.append(col)\n",
                "        except:\n",
                "            pass\n",
                "    # Time column\n",
                "    df_S1_ts_main_col = df_S1_ts_cols[ts_main_col_index]\n",
                "    serie_date_time_S1 = pd.to_datetime(df_S1[df_S1_ts_main_col], errors=\"coerce\")\n",
                "    log(\"Time index information:\", type=\"INFO\")\n",
                "    log(f\"Detected time column: '{df_S1_ts_main_col}'\", level=2, custom_icon=\"üìÖ\")\n",
                "    log(f\"Start date: {serie_date_time_S1.min()}\", level=2, custom_icon=\"üìÖ\")\n",
                "    log(f\"End date: {serie_date_time_S1.max()}\", level=2, custom_icon=\"üìÖ\")\n",
                "    log(f\"Total duration: {serie_date_time_S1.max() - serie_date_time_S1.min()}\", level=2, custom_icon=\"üìÖ\")\n",
                "    # Estimate frequency and granularity\n",
                "    serie_date_time_diff_S1 = serie_date_time_S1.diff().dropna()\n",
                "    if len(serie_date_time_diff_S1) > 0:\n",
                "        df_S1_most_common_delta = serie_date_time_diff_S1.mode()[0] # Most common interval\n",
                "        df_S1_smallest_delta = serie_date_time_diff_S1.min() # Minimal interval\n",
                "        df_S1_freq_ratio = (serie_date_time_diff_S1 == df_S1_most_common_delta).mean()\n",
                "        # Determine granularity\n",
                "        df_S1_seconds = df_S1_most_common_delta.total_seconds()\n",
                "        granularity = determine_granularity(df_S1_seconds)\n",
                "        log(f\"Most common interval: {df_S1_most_common_delta} (granularity: {granularity})\", level=2, custom_icon=\"üìÖ\")\n",
                "        log(f\"Smallest interval: {df_S1_smallest_delta}\", level=2, custom_icon=\"üìÖ\")\n",
                "        log(f\"Frequency consistency ratio: {df_S1_freq_ratio:.3f}\", level=2, custom_icon=\"üìÖ\")\n",
                "    else:\n",
                "        log(\"Not enough data points to estimate frequency\", type=\"WARNING\")\n",
                "    # Missing or irregular timestamps\n",
                "    missing_ratio = 1 - (serie_date_time_diff_S1 == most_common_delta).mean() if len(serie_date_time_diff_S1) > 0 else None\n",
                "    if missing_ratio is not None and missing_ratio > 0.10:\n",
                "        log(\"Irregular timestamps detected (missing or uneven intervals)\", type=\"WARNING\")\n",
                "        log(f\"Irregularity ratio: {missing_ratio:.2f}\", level=2, custom_icon=\"‚ö†Ô∏è\")\n",
                "    # Numeric metrics\n",
                "    numeric_cols = df_S1.select_dtypes(include=[\"number\"]).columns\n",
                "    log(\"Numeric metrics detected:\", type=\"INFO\")\n",
                "    for col in numeric_cols:\n",
                "        log(f\"{col}\", level=2, custom_icon=\"üìà\")\n",
                "    # Statistics for each metric\n",
                "    log(\"Basic statistics per numeric variable:\", type=\"INFO\")\n",
                "    display(df_S1[numeric_cols].describe().T)\n",
                "\n",
                "# -------------------------------\n",
                "# TABULAR DATASET\n",
                "# -------------------------------\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 1) EXPLORE DATAFRAME\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    log(\"Dataset detected as TABULAR\", custom_icon=\"üßÆ\")\n",
                "    # Print info\n",
                "    log(f\"Shape of the DataFrame: {df_S1.shape}\", type=\"INFO\")\n",
                "    log(\"Content of the DataFrame:\", type=\"INFO\")\n",
                "    display(df_S1.head(5))\n",
                "    log(\"Info of the DataFrame (dataType and non-null values):\", type=\"INFO\")\n",
                "    df_S1.info(verbose=True, show_counts=True)\n",
                "    # Ordered info (fewest non-null first)\n",
                "    ordered_info = pd.DataFrame({\n",
                "        \"Column\": df_S1.columns,\n",
                "        \"Non-Null Count\": df_S1.notnull().sum(),\n",
                "        \"Null Count\": df_S1.isnull().sum(),\n",
                "        \"Dtype\": df_S1.dtypes.astype(str)\n",
                "    }).sort_values(by=\"Non-Null Count\", ascending=True)\n",
                "    log(\"Ordered info by number of non-null values:\", type=\"INFO\")\n",
                "    display(ordered_info)\n",
                "    # Count unique attributes (unsorted)\n",
                "    df_S1_summary = pd.DataFrame({\n",
                "        \"Column\": df_S1.columns,\n",
                "        \"Unique_Count\": df_S1.nunique().values\n",
                "    })\n",
                "    log(\"DataFrame unique attributes (unsorted):\", type=\"INFO\")\n",
                "    display(df_S1_summary)\n",
                "    # Ordered summary (fewest unique first)\n",
                "    df_S1_summary_ordered = df_S1_summary.sort_values(by=\"Unique_Count\", ascending=True)\n",
                "    log(\"Ordered unique attributes (fewest unique first):\", type=\"INFO\")\n",
                "    display(df_S1_summary_ordered)\n",
                "    # Automatic Warning for high-uniqueness columns\n",
                "    unique_counts = df_S1.nunique()\n",
                "    high_unique_cols = unique_counts[unique_counts == len(df_S1)].index.tolist()\n",
                "    if len(high_unique_cols) > 0:\n",
                "        log(\"Consider dropping the following columns for having UNIQUE values for EVERY row:\", type=\"WARNING\")\n",
                "        for col in high_unique_cols:\n",
                "            log(f\"{col}\", level=2, custom_icon=\"üóëÔ∏è\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 2 - IDENTIFY TEXT COLUMN & METADATA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "# -------------------------------\n",
                "# INPUTS NEEDED IF dataset_type = \"TABULAR\"\n",
                "# -------------------------------\n",
                "cols_to_drop = []  # List of column names to drop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "-------------------------------\n",
                        "STEP 2) IDENTIFY TEXT COLUMN & METADATA\n",
                        "-------------------------------\n",
                        "\n",
                        "‚Ä¢ ‚ÑπÔ∏è Main text column selected: 'url'\n",
                        "‚Ä¢ ‚ÑπÔ∏è Additional non-text object columns detected:\n",
                        "   - üìÑ Column: 'is_spam'\n"
                    ]
                }
            ],
            "source": [
                "# Copy previous step data\n",
                "df_S2 = df_S1.copy()\n",
                "df_S2_ts_main_col = df_S1_ts_main_col if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# -------------------------------\n",
                "# NLP DATASET\n",
                "# -------------------------------\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 2) IDENTIFY TEXT COLUMN & METADATA\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    # Identify main text column\n",
                "    main_text_col_S2 = text_cols[0]\n",
                "    log(f\"Main text column selected: '{main_text_col_S2}'\", type=\"INFO\")\n",
                "    # Check for additional object columns\n",
                "    extra_object_cols = [c for c in df_S2.columns if c not in text_cols]\n",
                "    if len(extra_object_cols) > 0:\n",
                "        log(\"Additional non-text object columns detected:\", type=\"INFO\")\n",
                "        for col in extra_object_cols:\n",
                "            log(f\"Column: '{col}'\", level=2, custom_icon=\"üìÑ\")\n",
                "    else:\n",
                "        log(\"No additional metadata columns detected\", type=\"INFO\")\n",
                "    # Warn if more than one text-like column exists\n",
                "    if len(text_cols) > 1:\n",
                "        log(\"Multiple text-like columns detected. Consider selecting only one for preprocessing\", type=\"WARNING\")\n",
                "        for col in text_cols:\n",
                "            log(f\"Column: '{col}'\", level=2, custom_icon=\"üìù\")\n",
                "\n",
                "# -------------------------------\n",
                "# TIME-SERIES DATASET\n",
                "# -------------------------------\n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 2) BUILD TIME-SERIES\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    # Identify the temporal column\n",
                "    if len(df_S2_ts_main_col) > 0:\n",
                "        log(f\"Detected temporal column: '{df_S2_ts_main_col}'\", type=\"FOUND\")\n",
                "    else:\n",
                "        log(\"No temporal column found ‚Üí cannot build time index\", type=\"ERROR\")\n",
                "        df_S2_ts_main_col = None\n",
                "    # Stop if no datetime column exists\n",
                "    if df_S2_ts_main_col is None:\n",
                "        raise ValueError(\"No datetime column found ‚Üí cannot build time index.\")\n",
                "    # Drop the column if still present as normal column\n",
                "    if df_S2_ts_main_col in df_S2.columns:\n",
                "        df_S2 = df_S2.drop(columns=[df_S2_ts_main_col])\n",
                "    # Copy previous time-series\n",
                "    serie_date_time_S2 = serie_date_time_S1.copy()\n",
                "    # Sort by datetime just in case\n",
                "    serie_date_time_S2 = serie_date_time_S2.sort_values()\n",
                "    # Assign the datetime index\n",
                "    df_S2.index = serie_date_time_S2\n",
                "    # Make sure the index has a name\n",
                "    df_S2.index.name = df_S2_ts_main_col\n",
                "    # Show preview\n",
                "    log(f\"Indexed DataFrame by '{df_S2_ts_main_col}'\", type=\"INFO\")\n",
                "    log(\"Preview of time-indexed DataFrame:\", type=\"INFO\")\n",
                "    display(df_S2.head(5))\n",
                "    # Extract numeric target series\n",
                "    df_S2_numeric_cols = df_S2.select_dtypes(include=[\"number\"]).columns\n",
                "    if len(df_S2_numeric_cols) > 0:\n",
                "        # Pick the first numeric column as the time-series\n",
                "        df_S2_numeric_target_col = df_S2_numeric_cols[0]\n",
                "        df_timeseries_S2 = df_S2[df_S2_numeric_target_col].dropna()\n",
                "        log(f\"Extracted target time-series '{df_S2_numeric_target_col}'\", type=\"SUCCESS\")\n",
                "        display(df_timeseries_S2.head(5))\n",
                "        fig, axis = plt.subplots(figsize = (figWidth_unit, figHeight_unit))\n",
                "        sns.lineplot(data = df_timeseries_S2)\n",
                "        plt.grid(True)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "    else:\n",
                "        log(\"No numeric metrics detected to extract as the main time-series\", type=\"ERROR\")\n",
                "\n",
                "# -------------------------------\n",
                "# TABULAR DATASET\n",
                "# -------------------------------\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 2) SELECT RELEVANT ATTRIBUTES\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "     # Drop non-relevant attributes\n",
                "    df_S2=df_S2.drop(labels=cols_to_drop, axis =1)\n",
                "    # Print results\n",
                "    log(\"Non-Relevant attributes have been dropped\", type=\"SUCCESS\")\n",
                "    log(f\"Previous df's columns: {len(df_S1.columns)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Current df's columns: {len(df_S2.columns)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Final DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "    display(df_S2.head())\n",
                "    # Count attributes\n",
                "    df_S2_summary = pd.DataFrame({\n",
                "        \"Column\": df_S2.columns,\n",
                "        \"Unique_Count\": df_S2.nunique().values\n",
                "    })\n",
                "    log(\"Final DataFrame unique attributes:\", level=2, type=\"INFO\")\n",
                "    display(df_S2_summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 3 - REMOVE DUPLICATES"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "# -------------------------------\n",
                "# INPUTS NEEDED IF dataset_type = \"TIME-SERIES\"\n",
                "# -------------------------------\n",
                "# Threshold to determine seasonal component type (multiplicative or additive)\n",
                "seasonal_component_type_threshold = 0.3 # if amplitude/abs(mean_val) > threshold -> \"multiplicative\"\n",
                "# Thresholds to detect strong seasonal (both need to be higher than thresholds)\n",
                "strong_seasonal_threshold_for_acf = 0.6 # ACF at the seasonal period\n",
                "strong_seasonal_threshold_for_var_ratio = 0.5 # Variance ratio: Var(seasonal) / Var(original)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "-------------------------------\n",
                        "STEP 3) REMOVE DUPLICATES\n",
                        "-------------------------------\n",
                        "\n",
                        "‚Ä¢ ‚ö†Ô∏è Previous DataFrame contained 630 duplicates that have been dropped:\n",
                        "   - ‚ÑπÔ∏è Previous DataFrame shape: (2999, 2)\n",
                        "   - ‚ÑπÔ∏è Current DataFrame shape: (2369, 2)\n",
                        "   - ‚ÑπÔ∏è These are the dropped duplicates:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>url</th>\n",
                            "      <th>is_spam</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>60</th>\n",
                            "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>61</th>\n",
                            "      <td>https://www.hvper.com/</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>62</th>\n",
                            "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>64</th>\n",
                            "      <td>https://briefingday.com/fan</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>113</th>\n",
                            "      <td>https://briefingday.com/fan</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2971</th>\n",
                            "      <td>https://www.cnbc.com/2020/06/29/stock-market-f...</td>\n",
                            "      <td>False</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2972</th>\n",
                            "      <td>https://thehustle.co/account/</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2973</th>\n",
                            "      <td>https://thehustle.co/</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2979</th>\n",
                            "      <td>https://www.bloomberg.com/tosv2.html</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2995</th>\n",
                            "      <td>https://www.youtube.com/watch</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>630 rows √ó 2 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                    url  is_spam\n",
                            "60    https://briefingday.us8.list-manage.com/unsubs...     True\n",
                            "61                               https://www.hvper.com/     True\n",
                            "62                   https://briefingday.com/m/v4n3i4f3     True\n",
                            "64                          https://briefingday.com/fan     True\n",
                            "113                         https://briefingday.com/fan     True\n",
                            "...                                                 ...      ...\n",
                            "2971  https://www.cnbc.com/2020/06/29/stock-market-f...    False\n",
                            "2972                      https://thehustle.co/account/     True\n",
                            "2973                              https://thehustle.co/     True\n",
                            "2979               https://www.bloomberg.com/tosv2.html     True\n",
                            "2995                      https://www.youtube.com/watch     True\n",
                            "\n",
                            "[630 rows x 2 columns]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Copy previous step data\n",
                "df_S3 = df_S2.copy()\n",
                "df_timeseries_S3 = df_timeseries_S2.copy() if dataset_type == \"TIME-SERIES\" else None\n",
                "df_S3_freq_ratio = df_S1_freq_ratio if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# -------------------------------\n",
                "# NLP DATASET\n",
                "# -------------------------------\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 3) REMOVE DUPLICATES\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    num_duplicates=df_S3.duplicated().sum()\n",
                "    if num_duplicates == 0:\n",
                "        df_S3=df_S3\n",
                "        log(\"Previous DataFrame does not contain duplicates:\", type=\"SUCCESS\")\n",
                "        log(f\"Previous DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current DataFrame shape: {df_S3.shape}\", level=2, type=\"INFO\")\n",
                "    else:\n",
                "        df_S3_duplicates=df_S3[df_S3.duplicated()]\n",
                "        df_S3=df_S3.drop_duplicates()\n",
                "        log(f\"Previous DataFrame contained \" + str(num_duplicates) + \" duplicates that have been dropped:\", type=\"WARNING\")\n",
                "        log(f\"Previous DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current DataFrame shape: {df_S3.shape}\", level=2, type=\"INFO\")\n",
                "        log(\"These are the dropped duplicates:\", level=2, type=\"INFO\")\n",
                "        display(df_S3_duplicates)\n",
                "\n",
                "# -------------------------------\n",
                "# TIME-SERIES DATASET\n",
                "# -------------------------------        \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 3) DECOMPOSING\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    # 1) Validate series regularity before decomposition\n",
                "    if df_S3_freq_ratio < freq_ratio_threshold:\n",
                "        raise ValueError(f\"Decomposition skipped due to Low frequency regularity (freq_ratio={df_S3_freq_ratio:.3f})\")\n",
                "    log(f\"Timestamp regularity OK (freq_ratio={df_S3_freq_ratio:.3f})\", level=1, type=\"SUCCESS\")\n",
                "    # 2) Detect period using ACF (primary robust method)\n",
                "    period_acf = infer_period_from_acf(df_timeseries_S3)\n",
                "    if period_acf is not None:\n",
                "        period_S3 = period_acf\n",
                "        log(f\"Seasonality detected via ACF ‚Üí period = {period_acf}\", level=1, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"No significant seasonality found via ACF\", level=1, type=\"WARNING\")\n",
                "        # 3) Fallback based on granularity (if ACF failed)\n",
                "        period_fallback = infer_period_from_granularity(granularity)\n",
                "        if period_fallback is None:\n",
                "            raise ValueError(\"Unable to infer any valid period. Decomposition skipped because it is impossible to infer any valid period\")\n",
                "        log(f\"Fallback period inferred: {period_fallback} (granularity={granularity})\", level=1, type=\"SUCCESS\")\n",
                "        period_S3 = period_fallback\n",
                "    # 4) Determine model: additive or multiplicative\n",
                "    seasonal_component_type_S3 = infer_seasonal_component_type(df_timeseries_S3, seasonal_component_type_threshold)\n",
                "    log(f\"Type of seasonal component selected: {seasonal_component_type_S3}\", level=1, type=\"SUCCESS\")\n",
                "    # 5) Perform decomposition\n",
                "    try:\n",
                "        decomposition_S3 = seasonal_decompose(x=df_timeseries_S3, model=seasonal_component_type_S3, period=period_S3)\n",
                "        trend_S3 = decomposition_S3.trend\n",
                "        seasonal_S3 = decomposition_S3.seasonal\n",
                "        residual_S3 = decomposition_S3.resid\n",
                "        log(\"Decomposition completed successfully\", level=1, type=\"SUCCESS\")\n",
                "    except Exception as e:\n",
                "        raise ValueError(f\"Decomposition failed: {e}\")\n",
                "    # 6) Compute seasonality strength metrics\n",
                "    strong_seasonality_S3, seasonality_metrics_S3 = assess_seasonality_strength(\n",
                "        original        = df_timeseries_S3,\n",
                "        seasonal        = seasonal_S3,\n",
                "        period          = period_S3,\n",
                "        acf_threshold   = strong_seasonal_threshold_for_acf,\n",
                "        var_ratio       = strong_seasonal_threshold_for_var_ratio\n",
                "    )\n",
                "    if strong_seasonality_S3:\n",
                "        log(f\"Strong seasonality detected (var_ratio={seasonality_metrics_S3[\"seasonal_var_ratio\"]:.3f}, acf={seasonality_metrics_S3[\"acf_at_period\"]:.3f})\", level = 1, type  = \"SUCCESS\")\n",
                "    else:\n",
                "        log(f\"Weak or no seasonality (var_ratio={seasonality_metrics_S3[\"seasonal_var_ratio\"]:.3f}, acf={seasonality_metrics_S3[\"acf_at_period\"]:.3f})\", level = 1, type  = \"WARNING\")\n",
                "    # 7) Plot decomposition\n",
                "    fig, axis = plt.subplots(figsize = (figWidth_unit, figHeight_unit))\n",
                "    sns.lineplot(data = df_timeseries_S3, color = \"blue\", label = \"Original Time-series\")\n",
                "    sns.lineplot(data = trend_S3, color = \"orange\", label = \"Trend\", linestyle = \"--\")\n",
                "    sns.lineplot(data = residual_S3, color = \"red\", label = \"Residual\")\n",
                "    sns.lineplot(data = seasonal_S3, color = \"green\", label = \"Seasonal\", linestyle = \"--\")\n",
                "    plt.grid(True)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# -------------------------------\n",
                "# TABULAR DATASET\n",
                "# -------------------------------\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 3) REMOVE DUPLICATES\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    num_duplicates=df_S3.duplicated().sum()\n",
                "    if num_duplicates == 0:\n",
                "        df_S3=df_S3\n",
                "        log(\"Previous DataFrame does not contain duplicates:\", type=\"SUCCESS\")\n",
                "        log(f\"Previous DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current DataFrame shape: {df_S3.shape}\", level=2, type=\"INFO\")\n",
                "    else:\n",
                "        df_S3_duplicates=df_S3[df_S3.duplicated()]\n",
                "        df_S3=df_S3.drop_duplicates()\n",
                "        log(f\"Previous DataFrame contained \" + str(num_duplicates) + \" duplicates that have been dropped:\", type=\"WARNING\")\n",
                "        log(f\"Previous DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current DataFrame shape: {df_S3.shape}\", level=2, type=\"INFO\")\n",
                "        log(\"These are the dropped duplicates:\", level=2, type=\"INFO\")\n",
                "        display(df_S3_duplicates)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 4 - PROPOSE TARGET VARIABLE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "# -------------------------------\n",
                "# INPUTS NEEDED IF dataset_type = \"NLP\"\n",
                "# -------------------------------\n",
                "# # Thresholds for target variable proposal\n",
                "max_num_classes = 50                                            # Recommended: 50 classes ‚Üí max accepted number of different classes\n",
                "min_points_yvar = 75                                            # Minimum points to be considered as target candidate (max = 100 points)\n",
                "# -------------------------------\n",
                "# INPUTS NEEDED IF dataset_type = \"TIME-SERIES\"\n",
                "# -------------------------------\n",
                "accepted_alpha_dickey_fuller = 0.05                             # Accepted error in the hypothesis\n",
                "# -------------------------------\n",
                "# INPUTS NEEDED IF dataset_type = \"TABULAR\"\n",
                "# -------------------------------\n",
                "var_type_proposal_threshold = 25.00                             # [%] Under this percentage of unique values, the attribute is proposed as CATEGORIC\n",
                "float_discrete_threshold = min(30, round(0.02 * len(df_S3)))    # Dynamic threshold for FLOAT to be considered DISCRETE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "-------------------------------\n",
                        "STEP 4) PROPOSE TARGET VARIABLE\n",
                        "-------------------------------\n",
                        "\n",
                        "‚Ä¢ ‚ÑπÔ∏è Main text column assumed as: 'url'\n",
                        "‚Ä¢ ‚ÑπÔ∏è Columns to be proposed as target variable:\n",
                        "   - üîç Column 'is_spam':\n",
                        "      ¬∑ ‚úÖ Acceptable number of classes: 2 (+35 points)\n",
                        "      ¬∑ ‚úÖ Short label length (avg 4.9 chars) (+30 points)\n",
                        "      ¬∑ ‚ùå Non-categorical dtype (bool)\n",
                        "      ¬∑ ‚úÖ Reasonable class balance (min class ratio ‚â• 1%) (+15 points)\n",
                        "      ¬∑ üìù \u001b[1mTotal score: 80/100 points\u001b[0m\n",
                        "\n",
                        "\n",
                        "‚Ä¢ ‚ÑπÔ∏è \u001b[1mProposed target variable: 'is_spam' (80/100 points)\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "# Copy previous step data\n",
                "df_S4 = df_S3.copy()\n",
                "main_text_col_S4 = main_text_col_S2 if dataset_type == \"NLP\" else None\n",
                "df_timeseries_S4 = df_timeseries_S3.copy() if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# -------------------------------\n",
                "# NLP DATASET\n",
                "# -------------------------------\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 4) PROPOSE TARGET VARIABLE\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    # Safety check: make sure we have at least one text column\n",
                "    if len(text_cols) == 0:\n",
                "        log(\"No text-like columns were previously detected: cannot propose a target variable for NLP\", type=\"ERROR\")\n",
                "        y_var_auto = None\n",
                "    else:\n",
                "        # Main text column\n",
                "        log(f\"Main text column assumed as: '{main_text_col_S4}'\", type=\"INFO\")\n",
                "        # List of non-text columns (potential candidates for target variable)\n",
                "        non_text_cols = []\n",
                "        for col in df_S4.columns:\n",
                "            if col != main_text_col_S4:\n",
                "                non_text_cols.append(col)\n",
                "        if len(non_text_cols) == 0:\n",
                "            log(\"No additional columns apart from the main text: this is likely an unsupervised NLP task\", type=\"WARNING\")\n",
                "            y_var_auto = None\n",
                "        else:\n",
                "            log(\"Columns to be proposed as target variable:\", type=\"INFO\")\n",
                "            candidate_scores = {}   # Store total score per candidate\n",
                "            # Iterate through candidate columns\n",
                "            for col in non_text_cols:\n",
                "                log(f\"Column '{col}':\", level=2, type=\"FOUND\")\n",
                "                # Skip empty columns\n",
                "                if len(df_S4[col].dropna()) == 0:\n",
                "                    continue\n",
                "                n_unique = df_S4[col].dropna().nunique()\n",
                "                avg_len = df_S4[col].dropna().astype(str).str.len().mean()\n",
                "                class_distribution = df_S4[col].dropna().value_counts(normalize=True)\n",
                "                score = 0\n",
                "                # Evidence 1) Reasonable number of unique classes\n",
                "                if n_unique <= max_num_classes:\n",
                "                    score += 35\n",
                "                    log(f\"Acceptable number of classes: {n_unique} (+35 points)\", level=3, type=\"SUCCESS\")\n",
                "                else:\n",
                "                    log(f\"Too many unique classes: {n_unique}\", level=3, type=\"ERROR\")\n",
                "                # Evidence 2) Short label length (typical for target columns)\n",
                "                if avg_len <= 20:\n",
                "                    score += 30\n",
                "                    log(f\"Short label length (avg {avg_len:.1f} chars) (+30 points)\", level=3, type=\"SUCCESS\")\n",
                "                else:\n",
                "                    log(f\"Labels are too long on average (avg {avg_len:.1f} chars)\", level=3, type=\"ERROR\")\n",
                "                # Evidence 3) Categorical / label-like data type\n",
                "                if str(df_S4[col].dtype) in [\"object\", \"category\"]:\n",
                "                    score += 20\n",
                "                    log(f\"Object/Categorical dtype (+20 points)\", level=3, type=\"SUCCESS\")\n",
                "                else:\n",
                "                    log(f\"Non-categorical dtype ({str(df_S4[col].dtype)})\", level=3, type=\"ERROR\")\n",
                "                # Evidence 4) Class balance\n",
                "                if class_distribution.min() >= 0.01:\n",
                "                    score += 15\n",
                "                    log(f\"Reasonable class balance (min class ratio ‚â• 1%) (+15 points)\", level=3, type=\"SUCCESS\")\n",
                "                else:\n",
                "                    log(f\"Some classes have less than 1% of samples\", level=3, type=\"ERROR\")\n",
                "                # Store results\n",
                "                candidate_scores[col] = score\n",
                "                log(f\"Total score: {candidate_scores[col]}/100 points\", level=3, custom_icon=\"üìù\", bold=True)\n",
                "            # Select best candidate\n",
                "            if len(candidate_scores) == 0:\n",
                "                print(\"\\n\")\n",
                "                log(\"No valid non-text columns found to be used as target: this may be an unsupervised NLP task\", type=\"WARNING\", bold=True)\n",
                "                y_var_auto = None\n",
                "            else:\n",
                "                y_var_auto = max(candidate_scores, key=candidate_scores.get)\n",
                "                best_score = candidate_scores[y_var_auto]\n",
                "                if best_score >= min_points_yvar:\n",
                "                    print(\"\\n\")\n",
                "                    log(f\"Proposed target variable: '{y_var_auto}' ({best_score}/100 points)\", type=\"INFO\", bold=True)\n",
                "                else:\n",
                "                    print(\"\\n\")\n",
                "                    log(\"No column reached the minimum score to be confidently proposed as target\", type=\"WARNING\", bold=True)\n",
                "                    log(f\"Best candidate was '{y_var_auto}' with {best_score}/100 points (below threshold {min_points_yvar})\", type=\"INFO\", bold=True)\n",
                "                    y_var_auto = None\n",
                "    \n",
                "# -------------------------------\n",
                "# TIME-SERIES DATASET\n",
                "# -------------------------------        \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 4) STACIONARY ANAYSIS\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    # Perform Dickey-Fuller test to check for stacionarity\n",
                "    series_Dickey_Fuller_results = test_stationarity(series=df_timeseries_S4)\n",
                "\n",
                "    if series_Dickey_Fuller_results[\"p-value\"] >= accepted_alpha_dickey_fuller:\n",
                "        log(f\"Dickey-Fuller test's results:\\n{series_Dickey_Fuller_results}\\n\", type=\"INFO\")\n",
                "        log(f\"Hyphotesis rejected:Time-series IS NOT stationary, recursive differenciation is carried out\\n\", level=1, type=\"WARNING\", bold = True)\n",
                "        # Peform recursively Dickley-Fuller test until the time-series becomes stacionary\n",
                "        df_stationary_timeseries_S4, diff_count_S4, series_recursive_Dickey_Fuller_results = make_stationary_recursive(\n",
                "            series=df_timeseries_S4,\n",
                "            alpha=accepted_alpha_dickey_fuller\n",
                "            )\n",
                "        log(f\"Recursive differenciation ({diff_count_S4} step/s) -> Dickey-Fuller test's results:\\n{series_recursive_Dickey_Fuller_results}\\n\", level=1, type=\"INFO\")\n",
                "\n",
                "        if series_recursive_Dickey_Fuller_results[\"p-value\"] >= accepted_alpha_dickey_fuller:\n",
                "            log(f\"Time-series cannot become stationary (after {diff_count_S4} differencing step/s)\", level=1, type=\"WARNING\", bold = True)\n",
                "        else:\n",
                "            log(f\"Time-series can become stationary (after {diff_count_S4} differencing step/s)\", level=1, type=\"SUCCESS\", bold = True)\n",
                "    else:\n",
                "        diff_count_S4= 0\n",
                "        log(f\"Hyphotesis accepted: time-series IS stationary, no need of differenciation\", level=1, type=\"SUCCESS\", bold = True)\n",
                "\n",
                "# -------------------------------\n",
                "# TABULAR DATASET\n",
                "# -------------------------------\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 4) CLASSIFY ATTRIBUTES AND TARGET VARIABLE\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    # List of columns\n",
                "    columns = df_S4.columns.tolist()\n",
                "    # Iterate through columns\n",
                "    category_var_auto = []\n",
                "    numeric_var_auto = []\n",
                "    for col in df_S4.columns:\n",
                "        total_rows = len(df_S4)\n",
                "        # Skip empty columns\n",
                "        if total_rows == 0:\n",
                "            continue\n",
                "        unique_count =  df_S4[col].dropna().nunique()\n",
                "        unique_ratio = unique_count / total_rows * 100\n",
                "        col_dtype = str(df_S4[col].dtype)\n",
                "        # Case 1: text-based columns\n",
                "        if col_dtype in [\"object\", \"category\"]:\n",
                "            category_var_auto.append(col)\n",
                "            continue\n",
                "        # Case 2: integer columns\n",
                "        if col_dtype.startswith(\"int\"):\n",
                "            if unique_ratio <= var_type_proposal_threshold:\n",
                "                category_var_auto.append(col)\n",
                "            else:\n",
                "                numeric_var_auto.append(col)\n",
                "            continue\n",
                "        # Case 3: float columns\n",
                "        if col_dtype.startswith(\"float\"):\n",
                "            if unique_ratio <= var_type_proposal_threshold:\n",
                "                category_var_auto.append(col)\n",
                "            else:\n",
                "                numeric_var_auto.append(col)\n",
                "            continue\n",
                "    # Print proposed Data Types\n",
                "    log(f\"Proposed CATEGORY Attributes: {category_var_auto}\", type=\"INFO\", bold=True)\n",
                "    log(f\"Proposed NUMERIC Attributes: {numeric_var_auto}\", type=\"INFO\", bold=True)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 5 - VARIABILITY ANAYSIS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_var = \"is_spam\" # Confirm target variable\n",
                "\n",
                "# -------------------------------\n",
                "# INPUTS NEEDED IF dataset_type = \"NLP\"\n",
                "# -------------------------------\n",
                "\n",
                "# -------------------------------\n",
                "# INPUTS NEEDED IF dataset_type = \"TIME-SERIES\"\n",
                "# -------------------------------\n",
                "\n",
                "# -------------------------------\n",
                "# INPUTS NEEDED IF dataset_type = \"TABULAR\"\n",
                "# -------------------------------\n",
                "if_target_is_binary_treat_as_categoric = True   # Confirm treatment for target variable\n",
                "make_plots_UNIVARIANT = True                    # Draw plots?\n",
                "category_combi_att = \"\"                         # Combination attribute for multivariant analysis (must be a CATEGORIC attribute)\n",
                "y_var_highlighting_color = \"green\"              # Color to highlight target variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚Ä¢ ‚ÑπÔ∏è \u001b[1mConfirmed TARGET Variable: is_spam -> CATEGORIC and BINARY\u001b[0m\n",
                        "-------------------------------\n",
                        "STEP 5) ??????????????????????\n",
                        "-------------------------------\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Copy previous step data\n",
                "df_S5 = df_S4.copy()\n",
                "residual_S5 = residual_S3 if dataset_type == \"TIME-SERIES\" else None\n",
                "seasonal_component_type_S5 = seasonal_component_type_S3 if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# Checking TARGET variable\n",
                "y_unique_values = df_S5[y_var].nunique()\n",
                "y_unique_ratio = y_unique_values / len(df_S5) * 100\n",
                "y_dtype = df_S5[y_var].dtype.kind\n",
                "y_col_dtype = str(df_S5[y_var].dtype)\n",
                "y_var_type = None\n",
                "# Case 1: text-based columns\n",
                "if y_col_dtype in [\"object\", \"category\"]:\n",
                "    y_var_type = \"CATEGORIC\"\n",
                "# Case 2: integer columns\n",
                "if y_col_dtype.startswith(\"int\"):\n",
                "    y_var_type = \"CATEGORIC\" if y_unique_ratio <= var_type_proposal_threshold else \"NUMERIC\"\n",
                "# Case 3: float columns\n",
                "if y_col_dtype.startswith(\"float\"):\n",
                "    y_var_type = \"CATEGORIC\" if y_unique_ratio <= var_type_proposal_threshold else \"NUMERIC\"\n",
                "# Set subtype\n",
                "if y_var_type == \"CATEGORIC\":\n",
                "    if y_unique_values == 2:\n",
                "        y_var_subtype = \"BINARY\"\n",
                "    elif y_unique_values > 2:\n",
                "        y_var_subtype = \"MULTICLASS\"\n",
                "    else:\n",
                "        y_var_subtype = \"CONSTANT\"\n",
                "else:\n",
                "    if y_unique_values == 2 and if_target_is_binary_treat_as_categoric:\n",
                "        y_var_type = \"CATEGORIC\"\n",
                "        y_var_subtype = \"BINARY\"\n",
                "    elif y_dtype in ['i', 'u']:\n",
                "        y_var_type = \"NUMERIC\"\n",
                "        y_var_subtype = \"DISCRETE\"\n",
                "    elif y_dtype == 'f' and y_unique_values < float_discrete_threshold:\n",
                "        y_var_type = \"NUMERIC\"\n",
                "        y_var_subtype = \"DISCRETE\"\n",
                "    else:\n",
                "        y_var_type = \"NUMERIC\"\n",
                "        y_var_subtype = \"CONTINUOUS\"\n",
                "log(\"Confirmed TARGET Variable: \" + y_var + \" -> \" + y_var_type + \" and \" + y_var_subtype, type=\"INFO\", bold=True)\n",
                "    \n",
                "# -------------------------------\n",
                "# NLP DATASET\n",
                "# -------------------------------\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 5) ??????????????????????\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "\n",
                "# -------------------------------\n",
                "# TIME-SERIES DATASET\n",
                "# -------------------------------        \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 5) VARIABILITY ANALYSIS\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    # Drop NaN values in residuals\n",
                "    residual_S5 = residual_S5.dropna()\n",
                "    # -------------------------------------------\n",
                "    # RULE A: Check for visible trend in residuals\n",
                "    # -------------------------------------------\n",
                "    # Compute simple linear regression on residual vs time index\n",
                "    x_index = np.arange(len(residual_S5))\n",
                "    # Fit linear regression slope\n",
                "    slope, intercept = np.polyfit(x_index, residual_S5.values, 1)\n",
                "    # Compute residual standard deviation\n",
                "    residual_std = np.std(residual_S5)\n",
                "    # If residuals are almost constant ‚Üí no trend by definition\n",
                "    if residual_std < 1e-8:\n",
                "        log(\"Residual's slope analysis: Residuals are almost constant ‚Üí no visible trend (good).\", type=\"SUCCESS\")\n",
                "    else:\n",
                "        # Define a tolerance based on std and a minimum absolute tolerance\n",
                "        if abs(slope) < max(residual_std * 0.01, 1e-6):\n",
                "            log(\"Residual's slope analysis: No visible trend detected (good).\", type=\"SUCCESS\")\n",
                "        else:\n",
                "            log(\"Residual's slope analysis: Trend detected in residuals (bad).\", type=\"WARNING\", bold=True)\n",
                "    # -------------------------------------------\n",
                "    # RULE B: Check periodicity using ACF\n",
                "    # -------------------------------------------\n",
                "    # If residuals are almost constant ‚Üí ACF cannot detect periodicity, assume GOOD\n",
                "    if residual_std < 1e-8:\n",
                "        log(\"Residual's ACF analysis: Residuals are almost constant ‚Üí no periodicity possible (good).\", level=1, type=\"SUCCESS\")\n",
                "    else:\n",
                "        # Compute ACF up to 40 lags safely\n",
                "        nlags = min(40, len(residual_S5) - 2)\n",
                "        acf_res = acf(residual_S5, nlags=nlags, fft=True, missing=\"drop\")\n",
                "        # Detect highest non-zero lag correlation\n",
                "        acf_res_no0 = acf_res[1:]\n",
                "        max_acf_lag = np.argmax(np.abs(acf_res_no0)) + 1\n",
                "        max_acf_value = acf_res[max_acf_lag]\n",
                "        # Log ACF\n",
                "        log(f\"Residual ACF strongest lag={max_acf_lag}, value={max_acf_value:.3f}\", level=1, type=\"INFO\")\n",
                "        # If ACF is NaN ‚Üí cannot infer periodicity ‚Üí assume GOOD\n",
                "        if np.isnan(max_acf_value):\n",
                "            log(\"Residual's ACF analysis: ACF cannot be computed reliably (likely constant residuals) ‚Üí no periodicity detected (good).\", level=1, type=\"SUCCESS\")\n",
                "        # Periodicity rule: if max ACF < 0.3 ‚Üí no meaningful periodicity\n",
                "        elif abs(max_acf_value) < 0.3:\n",
                "            log(\"Residual's ACF analysis: No periodic patterns detected (good).\", level=1, type=\"SUCCESS\")\n",
                "        else:\n",
                "            log(\"Residual's ACF analysis: Residuals show periodic patterns (bad).\", level=1, type=\"WARNING\", bold=True)\n",
                "    # -------------------------------------------\n",
                "    # 4) RULE C: Check that residuals are centered\n",
                "    # -------------------------------------------\n",
                "    # Set theoretical center depending on decomposition model\n",
                "    residual_center = 1.0 if seasonal_component_type_S5 == \"multiplicative\" else 0.0\n",
                "    # Compute mean and standard deviation of cleaned residuals\n",
                "    mean_res = residual_S5.mean()\n",
                "    sd_res = residual_S5.std()\n",
                "    # Define relative and absolute tolerances\n",
                "    threshold_center = max(sd_res * 0.05, 1e-6)\n",
                "    # Center rule: mean close to the expected center (0 additive, 1 multiplicative)\n",
                "    if abs(mean_res - residual_center) < threshold_center:\n",
                "        log(f\"Residual's center analysis: Residuals centered around expected center ({residual_center}) (good).\", level=1, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(f\"Residual's center analysis: Residuals not centered around expected center ({residual_center}) (bad).\", level=1, type=\"WARNING\")\n",
                "    # -------------------------------------------\n",
                "    # 5) RULE D: Check for randomness using Ljung‚ÄìBox test\n",
                "    # -------------------------------------------\n",
                "    # Center residuals for randomness tests\n",
                "    residual_S5_centered = residual_S5 - residual_center\n",
                "    # Compute standard deviation\n",
                "    sd_res_centered = residual_S5_centered.std()\n",
                "    # If residuals are almost constant ‚Üí cannot test randomness, but constant noise = GOOD\n",
                "    if sd_res_centered < 1e-8:\n",
                "        log(\"Residual's randomness analysis: Residuals are almost constant ‚Üí randomness cannot be tested, assumed random (good).\", level=1, type=\"SUCCESS\")\n",
                "    else:\n",
                "        # Define safe number of lags\n",
                "        safe_lag = min(10, len(residual_S5_centered) - 2)\n",
                "        # Compute Ljung‚ÄìBox p-value\n",
                "        ljung_box_p = acorr_ljungbox(residual_S5_centered, lags=[safe_lag], return_df=True)[\"lb_pvalue\"].iloc[0]\n",
                "        # Log p-value\n",
                "        log(f\"Ljung‚ÄìBox p-value (lag {safe_lag}) = {ljung_box_p:.4f}\", type=\"INFO\")\n",
                "        # Randomness rule: if p > 0.05 ‚Üí residuals behave like white noise\n",
                "        if ljung_box_p > 0.05:\n",
                "            log(\"Residual's randomness analysis: Residuals behave as random noise (good).\", type=\"SUCCESS\")\n",
                "        else:\n",
                "            log(\"Residual's randomness analysis: Residuals show correlation ‚Üí not white noise (bad).\", type=\"WARNING\")\n",
                "\n",
                "# -------------------------------\n",
                "# TABULAR DATASET\n",
                "# -------------------------------\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    print(\"-------------------------------\")\n",
                "    print(\"STEP 5 - UNIVARIABLE ANALYSIS\")\n",
                "    print(\"-------------------------------\\n\")\n",
                "    # Confirm attribute types\n",
                "    category_att = []\n",
                "    numeric_att = []\n",
                "    for att in category_var_auto:\n",
                "        if att != y_var:\n",
                "            category_att.append(att)\n",
                "    for att in numeric_var_auto:\n",
                "        if att != y_var:\n",
                "            numeric_att.append(att)\n",
                "    # Checking CATEGORY attributes\n",
                "    binary_att = []\n",
                "    multiclass_att = []\n",
                "    constant_att = []\n",
                "    for att in category_att:\n",
                "        att_unique_values = df_S4[att].nunique()\n",
                "        if att_unique_values == 2:\n",
                "            binary_att.append(att)\n",
                "        elif att_unique_values > 2:\n",
                "            multiclass_att.append(att)\n",
                "        else:\n",
                "            constant_att.append(att)\n",
                "    # Checking NUMERIC attributes\n",
                "    discrete_att = []\n",
                "    continuos_att = []\n",
                "    for att in numeric_att:\n",
                "        att_dtype = df_S4[att].dtype.kind\n",
                "        unique_count = df_S4[att].nunique()\n",
                "        if att_dtype in ['i', 'u']:\n",
                "            discrete_att.append(att)\n",
                "        elif att_dtype == 'f' and unique_count < float_discrete_threshold:\n",
                "            discrete_att.append(att)\n",
                "        else:\n",
                "            continuos_att.append(att)\n",
                "    # Print results\n",
                "    print(\"- ‚ÑπÔ∏è Confirmed CATEGORY Attributes:\")\n",
                "    print(\"   ‚Ü≥ BINARY: \" + str(binary_att))\n",
                "    print(\"   ‚Ü≥ MULTICLASS: \" + str(multiclass_att))\n",
                "    print(\"   ‚Ü≥ CONSTANT: \" + str(constant_att))\n",
                "    print(\"- ‚ÑπÔ∏è Confirmed NUMERIC Attributes: \" + str(numeric_att))\n",
                "    print(\"   ‚Ü≥ DISCRETE: \" + str(discrete_att))\n",
                "    print(\"   ‚Ü≥ CONTINUOUS: \" + str(continuos_att))\n",
                "\n",
                "    if not make_plots_UNIVARIANT:\n",
                "        log(\"UNIVARIABLE ANALYSIS is not printed, set make_plots_UNIVARIANT = True\", type=\"WARNING\")\n",
                "    else:\n",
                "        # Validation\n",
                "        if not category_att:\n",
                "            print(\"- ‚ÑπÔ∏è There are no CATEGORIC attributes in the DataFrame\")\n",
                "        elif category_combi_att in category_att:\n",
                "            print(\"- ‚úÖ Sucessfull verification: combination attribute \" +  category_combi_att + \" is CATEGORIC\")\n",
                "        elif category_combi_att in numeric_att:\n",
                "            raise ValueError(\"‚ùå Combination attribute \" +  category_combi_att + \" for multivariant analysis must be a CATEGORY attribute!\")\n",
                "        else:\n",
                "            raise ValueError(\"‚ùå Combination attribute \" +  category_combi_att + \" does not exist in the DataFrame\")\n",
                "        # Target highlighting styles\n",
                "        target_box_style = dict(facecolor='none', edgecolor=y_var_highlighting_color, linewidth=5)\n",
                "        target_title_style = dict(fontsize= plot_title_font_size, color=y_var_highlighting_color, fontweight='bold')\n",
                "        # -------------------------------------------\n",
                "        # CATEGORY VARIABLES (including target if categorical)\n",
                "        # -------------------------------------------\n",
                "        print(\"üè∑Ô∏è CATEGORY VARIABLES\")\n",
                "        if not category_att and y_var_type == \"NUMERIC\":\n",
                "            log(\"This type of plot is non applicable because there are not CATEGORIC variables in the DataFrame\", type=\"WARNING\")\n",
                "        else:    \n",
                "            var_to_plot = category_att.copy()\n",
                "            if y_var_type == \"CATEGORIC\" and y_var not in var_to_plot:\n",
                "                var_to_plot.insert(0, y_var)\n",
                "            # Figure\n",
                "            num_cols = 2\n",
                "            num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
                "            fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "            axes = axes.flatten()\n",
                "            idx = 0\n",
                "            for col in var_to_plot:\n",
                "                unique_count = df_S5[col].nunique()\n",
                "                if unique_count > num_values_to_plot:\n",
                "                    order = df_S5[col].value_counts().head(num_values_to_plot).index\n",
                "                else:\n",
                "                    order = df_S5[col].value_counts().index\n",
                "                # Countplot\n",
                "                sns.countplot(ax=axes[idx], data=df_S5, x=col, hue=col, palette=plot_palette, order=order, legend=False)\n",
                "                axes[idx].tick_params(axis='x', rotation=90, labelsize=plot_tick_font_size)\n",
                "                axes[idx].set_xlabel(\"\")\n",
                "                # Highlight target\n",
                "                if col == y_var:\n",
                "                    axes[idx].set_title(col, **target_title_style)\n",
                "                    axes[idx].add_patch(plt.Rectangle((0, 0), 1, 1, transform=axes[idx].transAxes, **target_box_style))\n",
                "                else:\n",
                "                    axes[idx].set_title(col, fontdict = {\"fontsize\": plot_title_font_size})\n",
                "                # Add truncated info\n",
                "                if unique_count > num_values_to_plot:\n",
                "                    msg = f\"There are {unique_count} values,\\nbut only {num_values_to_plot} have been plotted\"\n",
                "                    axes[idx].text(0.5, 0.9, msg, transform=axes[idx].transAxes, fontsize=plot_text_font_size, color=\"red\", ha=\"center\", va=\"top\", bbox=dict(facecolor=\"grey\", alpha=0.25, edgecolor=\"red\"))\n",
                "                idx += 1\n",
                "            # Hide unused axes\n",
                "            for j in range(idx, len(axes)):\n",
                "                axes[j].set_visible(False)\n",
                "            plt.tight_layout()\n",
                "            plt.show()\n",
                "        # -------------------------------------------\n",
                "        # NUMERIC VARIABLES (including target if numeric)\n",
                "        # -------------------------------------------\n",
                "        print(\"üî¢ NUMERIC VARIABLES\")\n",
                "        if not numeric_att and y_var_type == \"CATEGORIC\":\n",
                "            log(\"This type of plot is non applicable because there are not NUMERIC variables in the DataFrame\", type=\"WARNING\")\n",
                "        else: \n",
                "            var_to_plot = numeric_att.copy()\n",
                "            if y_var_type == \"NUMERIC\" and y_var not in var_to_plot:\n",
                "                var_to_plot.insert(0, y_var)\n",
                "            # Figure\n",
                "            num_cols = 2 \n",
                "            num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
                "            fig, axes = plt.subplots(nrows=num_rows * 2, ncols=num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows), gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
                "            var_idx = 0\n",
                "            for row in range(num_rows):\n",
                "                for col in range(num_cols):\n",
                "                    if var_idx >= len(var_to_plot):\n",
                "                        axes[row * 2, col].set_visible(False)\n",
                "                        axes[row * 2 + 1, col].set_visible(False)\n",
                "                        continue\n",
                "                    colname = var_to_plot[var_idx]\n",
                "                    # Histogram\n",
                "                    sns.histplot(ax=axes[row * 2, col], data=df_S5, x=colname, bins=num_bins)\n",
                "                    axes[row * 2, col].set_xlabel(\"\")\n",
                "                    # Boxplot\n",
                "                    sns.boxplot(ax=axes[row * 2 + 1, col], data=df_S5, x=colname)\n",
                "                    axes[row * 2 + 1, col].set_xlabel(\"\")\n",
                "                    # Highlight target\n",
                "                    if colname == y_var:\n",
                "                        axes[row * 2, col].set_title(colname, **target_title_style)\n",
                "                        axes[row * 2 + 1, col].set_title(colname, **target_title_style)\n",
                "                        axes[row * 2, col].add_patch(plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2, col].transAxes, **target_box_style))\n",
                "                        axes[row * 2 + 1, col].add_patch(plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2 + 1, col].transAxes, **target_box_style))\n",
                "                    else:\n",
                "                        axes[row * 2, col].set_title(colname, fontdict = {\"fontsize\": plot_title_font_size})\n",
                "                    var_idx += 1\n",
                "            plt.tight_layout()\n",
                "            plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 6 - AUTOCORRELATION ANALYSIS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"STEP 6) AUTOCORRELATION ANALYSIS\")\n",
                "\n",
                "# Copy previous time-series\n",
                "df_timeseries_S6 = df_timeseries_S3.copy()\n",
                "period_S6 = period_S3\n",
                "suggested_d  = diff_count_S4\n",
                "df_stacionary_timeseries_for_pacf_S6 = df_stationary_timeseries_S4.copy() # Use stationary series from STEP 4 to identify AR order\n",
                "\n",
                "# Set frequency for ARIMA based on granularity\n",
                "freq = get_freq_from_granularity(granularity)\n",
                "\n",
                "if freq is not None:\n",
                "    try:\n",
                "        df_timeseries_S6 = df_timeseries_S6.asfreq(freq)\n",
                "        log(f\"Applied pandas frequency '{freq}' to series for ARIMA modeling.\", level=1, type=\"INFO\")\n",
                "    except Exception as e:\n",
                "        log(f\"Could not apply frequency '{freq}': {e}\", level=1, type=\"WARNING\")\n",
                "else:\n",
                "    log(f\"No valid pandas freq for granularity='{granularity}', ARIMA frequency skipped.\", level=1, type=\"WARNING\")\n",
                "\n",
                "# If series is almost constant ‚Üí no meaningful ACF/PACF\n",
                "if df_timeseries_S6.std() < 1e-8:\n",
                "    log(\"ACF/PACF analysis: Time-series is almost constant ‚Üí no meaningful autocorrelation.\", level=1, type=\"WARNING\")\n",
                "\n",
                "else:\n",
                "    # Get recommended lag based on granularity\n",
                "    recommended_lag = get_recommended_lag(granularity)\n",
                "    # Limit by available data\n",
                "    safe_lag = min(recommended_lag, len(df_timeseries_S6) - 2)\n",
                "    # Log chosen lag\n",
                "    log(f\"Used safe_lag = {safe_lag} (recommended={recommended_lag}, granularity={granularity})\", level=1, type=\"INFO\")\n",
                "    # Compute confidence (95%) limit for significance bands\n",
                "    conf_limit = 1.96 / np.sqrt(len(df_timeseries_S6))\n",
                "\n",
                "    # -------------------------------------------\n",
                "    # ACF BEHAVIOUR\n",
                "    # -------------------------------------------\n",
                "    # Compute numerical ACF values\n",
                "    acf_vals = acf(df_timeseries_S6, nlags=safe_lag, fft=True, missing=\"drop\")\n",
                "    # Build list of significant ACF lags\n",
                "    significant_acf_lags = []\n",
                "    seasonal_peaks_S6 = []\n",
                "    for lag in range(1, len(acf_vals)):\n",
                "        val = acf_vals[lag]\n",
                "        if abs(val) > conf_limit:\n",
                "            significant_acf_lags.append(lag)\n",
                "    if len(significant_acf_lags) == 0:\n",
                "        log(\"ACF analysis: No significant autocorrelation detected ‚Üí series close to white noise.\", level=1, type=\"INFO\")\n",
                "    else:\n",
                "        log(f\"ACF analysis: Significant autocorrelation at lags {significant_acf_lags}.\", level=1, type=\"INFO\")\n",
                "        # Check short-lag ACF persistence (trend indicator)\n",
                "        short_lags = []\n",
                "        for lag in significant_acf_lags:\n",
                "            if lag <= min(get_short_lag_cutoff(granularity), safe_lag):\n",
                "                short_lags.append(lag)\n",
                "        if len(short_lags) > 0:\n",
                "            log(\"ACF analysis: High short-lag autocorrelation ‚Üí possible trend or strong persistence.\", level=1, type=\"INFO\")\n",
                "        # Check for seasonal multiples\n",
                "        if (period_S6 is not None) and (period_S6 <= safe_lag):\n",
                "            seasonal_peaks = []\n",
                "            max_k = safe_lag // period_S6\n",
                "            for k in range(1, max_k + 1):\n",
                "                lag = k * period_S6\n",
                "                if lag in significant_acf_lags:\n",
                "                    seasonal_peaks.append(lag)\n",
                "            seasonal_peaks_clean_print = []        \n",
                "            for l in seasonal_peaks:\n",
                "                seasonal_peaks_clean_print.append(int(l))\n",
                "            if len(seasonal_peaks) > 0:\n",
                "                seasonal_peaks_S6 = seasonal_peaks.copy()\n",
                "                log(f\"ACF analysis: Significant seasonal peaks at lags {seasonal_peaks_clean_print} ‚Üí strong seasonality.\", level=1, type=\"SUCCESS\")\n",
                "\n",
                "    # -------------------------------------------\n",
                "    # PACF BEHAVIOUR\n",
                "    # -------------------------------------------\n",
                "    # Compute numerical PACF values\n",
                "    pacf_vals = pacf(df_stacionary_timeseries_for_pacf_S6, nlags=safe_lag, method=\"ywm\")\n",
                "    # Build list of significant PACF lags\n",
                "    significant_pacf_lags = []\n",
                "    for lag in range(1, len(pacf_vals)):\n",
                "        val = pacf_vals[lag]\n",
                "        if abs(val) > conf_limit:\n",
                "            significant_pacf_lags.append(lag)\n",
                "    if len(significant_pacf_lags) == 0:\n",
                "        suggested_p = 0\n",
                "        log(\"PACF analysis: No significant partial autocorrelation detected.\", level=1, type=\"INFO\")\n",
                "    else:\n",
                "        log(f\"PACF analysis: Significant PACF lags detected {significant_pacf_lags}.\", level=1, type=\"INFO\")\n",
                "        # Keep non-seasonal PACF lags\n",
                "        non_seasonal_pacf = []\n",
                "        for lag in significant_pacf_lags:\n",
                "            if (period_S6 is None) or (lag % period_S6 != 0):\n",
                "                non_seasonal_pacf.append(lag)\n",
                "        if len(non_seasonal_pacf) > 0:\n",
                "            suggested_p = non_seasonal_pacf[0]\n",
                "            log(f\"PACF analysis: First significant non-seasonal lag = {suggested_p} ‚Üí candidate AR order p ‚âà {suggested_p}.\", level=1, type=\"INFO\")\n",
                "        else:\n",
                "            suggested_p = 0\n",
                "    # -------------------------------------------\n",
                "    # MODEL ORDER SUGGESTION (AR / MA)\n",
                "    # -------------------------------------------\n",
                "    if len(significant_acf_lags) > 0:\n",
                "        suggested_q = significant_acf_lags[0]\n",
                "    else:\n",
                "        suggested_q = 0\n",
                "    # Start from suggested values\n",
                "    candidate_orders = [(suggested_p, suggested_d, suggested_q)]\n",
                "    # If suggested_p = 0, also try p = 1 as alternative\n",
                "    if suggested_p == 0:\n",
                "        candidate_orders.append((1, suggested_d, suggested_q))\n",
                "    best_aic = np.inf\n",
                "    best_order = None\n",
                "    for (p_try, d_try, q_try) in candidate_orders:\n",
                "        try:\n",
                "            model_try = ARIMA(df_timeseries_S6, order=(p_try, d_try, q_try))\n",
                "            result_try = model_try.fit()\n",
                "            if result_try.aic < best_aic:\n",
                "                best_aic = result_try.aic\n",
                "                best_order = (p_try, d_try, q_try)\n",
                "        except Exception as e:\n",
                "            log(f\"ARIMA({p_try},{d_try},{q_try}) could not be fitted: {e}\", type=\"WARNING\")\n",
                "\n",
                "    if best_order is not None:\n",
                "        suggested_p, suggested_d, suggested_q = best_order\n",
                "        log(f\"Final ARIMA order suggestion: (p,d,q)=({suggested_p},{suggested_d},{suggested_q}) after AIC-checked refinement (best AIC={best_aic:.2f}).\", level=1, type=\"INFO\", bold=True)\n",
                "\n",
                "    # -------------------------------------------\n",
                "    # PLOT ACF\n",
                "    # -------------------------------------------\n",
                "    fig_acf, ax_acf = plt.subplots(nrows=1, ncols=1, figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
                "    plot_acf(df_timeseries_S6, lags=safe_lag, ax=ax_acf)\n",
                "    ax_acf.set_title(label=\"Autocorrelation Function (ACF)\", fontsize=plot_title_font_size)\n",
                "    ax_acf.set_xlabel(xlabel=\"Lag\", fontsize=plot_label_font_size)\n",
                "    ax_acf.set_ylabel(ylabel=\"Autocorrelation\", fontsize=plot_label_font_size)\n",
                "    ax_acf.tick_params(labelsize=plot_tick_font_size)\n",
                "    ax_acf.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "    # Build manual legend to avoid statsmodels overwriting handles\n",
                "    handles, labels = [], []\n",
                "    # Highlight short-lag zone (trend indicator)\n",
                "    short_cutoff = min(get_short_lag_cutoff(granularity), safe_lag)\n",
                "    ax_acf.axvspan(\n",
                "        xmin=1,\n",
                "        xmax=short_cutoff,\n",
                "        color=\"lightblue\",\n",
                "        alpha=0.35,\n",
                "        label=f\"Short-lag zone (‚â§ {short_cutoff})\"\n",
                "    )\n",
                "    handles.append(plt.Line2D([0], [0], color=\"lightblue\", linewidth=10, alpha=0.35))\n",
                "    labels.append(f\"Short-lag zone (‚â§ {short_cutoff})\")\n",
                "    # Mark seasonal period (if applicable)\n",
                "    if (period_S6 is not None) and (period_S6 <= safe_lag):\n",
                "        ax_acf.axvline(\n",
                "            x=period_S6,\n",
                "            color=\"orange\",\n",
                "            linestyle=\"-\",\n",
                "            linewidth=4.0,\n",
                "            alpha=0.8,\n",
                "            label=f\"Seasonal period (lag={period_S6})\"\n",
                "        )\n",
                "        handles.append(plt.Line2D([0], [0], color=\"orange\", linewidth=4))\n",
                "        labels.append(f\"Seasonal period (lag={period_S6})\")\n",
                "    # Mark significant ACF lags (points)\n",
                "    if len(significant_acf_lags) > 0:\n",
                "        ax_acf.scatter(\n",
                "            significant_acf_lags,\n",
                "            [acf_vals[lag] for lag in significant_acf_lags],\n",
                "            color=\"blue\",\n",
                "            s=100,\n",
                "            label=\"Significant lags\"\n",
                "        )\n",
                "        handles.append(plt.Line2D([0], [0], marker=\"o\", color=\"blue\", linestyle=\"None\"))\n",
                "        labels.append(\"Significant lags\")\n",
                "    # Info text box\n",
                "    ax_acf.text(\n",
                "        0.98, 0.02,\n",
                "        f\"safe_lag = {safe_lag}\\n\"\n",
                "        f\"granularity = {granularity}\",\n",
                "        ha='right',\n",
                "        va='bottom',\n",
                "        transform=ax_acf.transAxes,\n",
                "        fontsize=plot_text_font_size,\n",
                "        bbox=dict(boxstyle=\"round\", fc=\"white\", alpha=0.6)\n",
                "    )\n",
                "    # Show legend and plot\n",
                "    ax_acf.legend(handles, labels, loc=\"upper right\", fontsize=plot_text_font_size)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    # -------------------------------------------\n",
                "    # PLOT PACF\n",
                "    # -------------------------------------------\n",
                "    fig_pacf, ax_pacf = plt.subplots(nrows=1, ncols=1, figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
                "    plot_pacf(df_timeseries_S6, lags=safe_lag, ax=ax_pacf)\n",
                "    ax_pacf.set_title(label=\"Partial Autocorrelation Function (PACF)\", fontsize=plot_title_font_size)\n",
                "    ax_pacf.set_xlabel(xlabel=\"Lag\", fontsize=plot_label_font_size)\n",
                "    ax_pacf.set_ylabel(ylabel=\"Partial autocorrelation\", fontsize=plot_label_font_size)\n",
                "    ax_pacf.tick_params(labelsize=plot_tick_font_size)\n",
                "    ax_pacf.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "    # Build manual legend to avoid statsmodels overwriting handles\n",
                "    handles, labels = [], []\n",
                "    # Mark significant PACF lags (points)\n",
                "    if len(significant_pacf_lags) > 0:\n",
                "        ax_pacf.scatter(\n",
                "            significant_pacf_lags,\n",
                "            [pacf_vals[lag] for lag in significant_pacf_lags],\n",
                "            color=\"blue\",\n",
                "            s=100,\n",
                "            label=\"Significant lags\"\n",
                "        )\n",
                "        handles.append(plt.Line2D([0], [0], marker=\"o\", color=\"blue\", linestyle=\"None\"))\n",
                "        labels.append(\"Significant lags\")\n",
                "    # Info text box\n",
                "    ax_pacf.text(\n",
                "        0.98, 0.02,\n",
                "        f\"safe_lag = {safe_lag}\",\n",
                "        ha='right',\n",
                "        va='bottom',\n",
                "        transform=ax_pacf.transAxes,\n",
                "        fontsize=plot_text_font_size,\n",
                "        bbox=dict(boxstyle=\"round\", fc=\"white\", alpha=0.6)\n",
                "    )\n",
                "    # Show legend and plot\n",
                "    ax_pacf.legend(handles, labels, loc=\"upper right\", fontsize=plot_text_font_size)\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 7 - PREDICTION MODEL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prediction_horizon = 360"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"STEP 7) PREDICTION MODEL\")\n",
                "\n",
                "# Copy previous time-series\n",
                "df_timeseries_S7 = df_timeseries_S6.copy()\n",
                "period_S7 = period_S6\n",
                "seasonal_peaks_S7 = seasonal_peaks_S6\n",
                "\n",
                "# ======================================================\n",
                "#  ARIMA MODEL WITH SUGGESTED VALUES\n",
                "# ======================================================\n",
                "# Instance ARIMA model\n",
                "arima_model = ARIMA(endog = df_timeseries_S7, order = (suggested_p, suggested_d, suggested_q))\n",
                "# Train ARIMA model\n",
                "arima_result = arima_model.fit()\n",
                "# Predict with trained ARIMA model\n",
                "prediction_ARIMA = arima_result.predict(start  = len(df_timeseries_S7), end= len(df_timeseries_S7) + prediction_horizon)\n",
                "\n",
                "# ======================================================\n",
                "#  AUTO ARIMA\n",
                "# ======================================================\n",
                "# Decide seasonal behaviour for auto_arima\n",
                "auto_arima_m = get_auto_arima_m(period=period_S7,seasonal_peaks = seasonal_peaks_S6)\n",
                "if auto_arima_m > 1:\n",
                "    auto_arima_seasonal = True\n",
                "else:\n",
                "    auto_arima_seasonal = False\n",
                "# Instance AUTO ARIMA model\n",
                "auto_arima_model = auto_arima(y=df_timeseries_S7, seasonal = auto_arima_seasonal, trace = False, m = auto_arima_m)\n",
                "# Retrieve orders\n",
                "auto_p, auto_d, auto_q = auto_arima_model.order\n",
                "# Predict with trained AUTO ARIMA model\n",
                "prediction_AUTO_ARIMA = auto_arima_model.predict(prediction_horizon)\n",
                "\n",
                "# ======================================================\n",
                "#  COMPARISON\n",
                "# ======================================================\n",
                "log(f\"ARIMA (manual): selected order (p,d,q)=({suggested_p},{suggested_d},{suggested_q})\", level=1, type=\"INFO\")\n",
                "log(f\"AUTO-ARIMA: selected order (p,d,q)=({auto_p},{auto_d},{auto_q}) with seasonal={auto_arima_seasonal}, m={auto_arima_m}\", level=1, type=\"INFO\")\n",
                "\n",
                "if (suggested_p == auto_p) and (suggested_d == auto_d) and (suggested_q == auto_q):\n",
                "    log(f\"ARIMA vs AUTO-ARIMA: Orders MATCH\", level=1, type=\"SUCCESS\", bold=True)\n",
                "else:\n",
                "    log(f\"ARIMA vs AUTO-ARIMA: Orders DO NOT MATCH\", level=1, type=\"WARNING\", bold=True)\n",
                "\n",
                "# ======================================================\n",
                "#  FORECAST PLOT ‚Äî ARIMA\n",
                "# ======================================================\n",
                "fig_arima, ax_arima = plt.subplots(nrows=1, ncols=1, figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
                "# Plot original series\n",
                "ax_arima.plot(df_timeseries_S7, label=\"Original Time-series\")\n",
                "# Plot ARIMA forecast\n",
                "ax_arima.plot(prediction_ARIMA, label=\"Forecast (ARIMA)\", color=\"red\", linewidth=5, linestyle=\"dashed\")\n",
                "# Title, labels, ticks and legend\n",
                "ax_arima.set_title(label=f\"Forecast with ARIMA(p={suggested_p}, d={suggested_d}, q={suggested_q})\", fontsize=plot_title_font_size)\n",
                "ax_arima.set_xlabel(xlabel=df_timeseries_S7.index.name, fontsize=plot_label_font_size)\n",
                "ax_arima.set_ylabel(ylabel=df_timeseries_S7.name, fontsize=plot_label_font_size)\n",
                "ax_arima.tick_params(labelsize=plot_tick_font_size)\n",
                "ax_arima.legend(fontsize=plot_text_font_size)\n",
                "ax_arima.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "# Show plot\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# ======================================================\n",
                "#  FORECAST PLOT ‚Äî AUTO ARIMA\n",
                "# ======================================================\n",
                "fig_auto, ax_auto_arima = plt.subplots(nrows=1, ncols=1, figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
                "# Plot original series\n",
                "ax_auto_arima.plot(df_timeseries_S7, label=\"Original Time-series\")\n",
                "# Plot AUTO-ARIMA forecast\n",
                "ax_auto_arima.plot(prediction_AUTO_ARIMA, label=\"Forecast (AUTO-ARIMA)\", color=\"green\", linewidth=5, linestyle=\"dashed\")\n",
                "# Title, labels, ticks and legend\n",
                "ax_auto_arima.set_title(label=f\"Forecast with AUTO-ARIMA(p={auto_p}, d={auto_d}, q={auto_q})\", fontsize=plot_title_font_size)\n",
                "ax_auto_arima.set_xlabel(xlabel=df_timeseries_S7.index.name, fontsize=plot_label_font_size)\n",
                "ax_auto_arima.set_ylabel(ylabel=df_timeseries_S7.name, fontsize=plot_label_font_size)\n",
                "ax_auto_arima.tick_params(labelsize=plot_tick_font_size)\n",
                "ax_auto_arima.legend(fontsize=plot_text_font_size)\n",
                "ax_auto_arima.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "# Show plot\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 8 - SAVE MODELS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models_output_path = \"../models/\"   # Folder where models will be saved\n",
                "rev_to_use = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"STEP 8) SAVE MODELS\")\n",
                "\n",
                "arima_filename = models_output_path + f\"ARIMA_rev{rev_to_use}.sav\"\n",
                "dump(arima_result, open(arima_filename, \"wb\"))\n",
                "log(f\"Model saved: {arima_filename}\", level=1, type=\"SUCCESS\")\n",
                "\n",
                "auto_arima_filename = models_output_path + f\"AUTO_MANUAL_rev{rev_to_use}.sav\"\n",
                "dump(auto_arima_model, open(auto_arima_filename, \"wb\"))\n",
                "log(f\"Model saved: {auto_arima_filename}\", level=1, type=\"SUCCESS\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
